{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4988daf4-346a-4a15-b8f8-4802ccdfb87b"
    }
   },
   "source": [
    "# Connectivity\n",
    "\n",
    "Traditional multivariate fMRI techniques focus on the information present in patterns of activity in localized regions (ROIs or searchlights). Sometimes, the relevant information may be represented across a network of brain regions and thus would not be identified via ROI analysis or searchlights. Functional connectivity measures help examine information at a global level, in regions that are far apart, by focusing on network interaction rather than spatial localization. When performing connectivity analyses, BOLD timeseries are compared across regions (usually with correlation) and the strength of the relationship determines their functional connectivity. By including or excluding stimulus/task variables, we can study the modulation of connectivity by different cognitive states.\n",
    "\n",
    "We are going to cover connectivity analysis over the next two notebooks, starting in this notebook with more basic ROI-level, task-based, and background connectivity. The next notebook will cover whole-brain connectivity analyses.  \n",
    "\n",
    "\n",
    "## Goal of this script\n",
    ">Learn how to run seed-based connectivity analyses.  \n",
    ">Learn to use an atlas to get parcels and define seeds.  \n",
    ">Investigate how attention modulates connectivity.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "[1. Data loading](#load_data)  \n",
    ">[1.1 Create stimulus labels and time-shift](#time_shift)  \n",
    ">[1.2 Examine the header file](#header_info)  \n",
    ">[1.3 Plot the stimulus labels](#stim_labels)  \n",
    ">[1.4 Mask and extract the whole-brain data](#mask)\n",
    "\n",
    "[2. Create a seed](#seed)\n",
    ">[2.1 Create a spherical ROI](#ROI_sphere)    \n",
    ">[2.2 Plot the bold signal for the mask](#mask_signal)\n",
    "\n",
    "[3. Compute the correlation matrix](#correlation_matrix)  \n",
    ">[3.1 Plot the seed correlations](#plot_seed)\n",
    "\n",
    "[4. Creating a seed from an atlas](#ROI_atlas)  \n",
    ">[4.1 Compute connectivity across parcels](#parcel_corr) \n",
    "\n",
    "[5. Background connectivity](#back_conn)  \n",
    "\n",
    "[6. Group analyses](#group)\n",
    "### Exercises:  \n",
    ">[1](#ex1)   [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)  [7](#ex7)  [8](#ex8)  [9](#ex9)  [10](#ex10)   [11](#ex11)  \n",
    "\n",
    ">[Novel contribution](#novel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1f7f9d75-833f-410f-8988-58c1618fa753"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker, NiftiLabelsMasker\n",
    "from nilearn import plotting\n",
    "from nilearn import datasets\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from scipy import stats\n",
    "from scipy.ndimage.measurements import center_of_mass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import brainiak.utils.fmrisim as sim\n",
    "\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "from nilearn import input_data\n",
    "import time\n",
    "from utils import shift_timing\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(palette=\"colorblind\",style = 'white', context='notebook', rc={\"lines.linewidth\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** The dataset we will be using was collected as part of a study on attention and memory ([Hutchinson et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/26439270)). Below find the README file describing the dataset.\n",
    "This dataset has been preprocessed with motion correction and linear detrending. Subjects were asked to attend to a scene on the right or on the left in each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention and connectivity**\n",
    "\n",
    "Our brain is constantly bombarded by sensory information from the world. Attention refers to the set of cognitive processes that filter this input based on what is salient (e.g., a police siren) and what is relevant (e.g., faces when looking for a friend). Attention biases our behavior and conscious awareness towards these properties of the world and reduces distraction by other properties. At the neural level, attention is controlled by parietal and frontal cortices [(Corbetta and Shulman, 2002)](https://www.ncbi.nlm.nih.gov/pubmed/11994752), which modulate processing in sensory systems, enhancing attended information and suppressing unattended information [(Noudoost et al., 2010)](https://www.ncbi.nlm.nih.gov/pubmed/20303256). Thus, to study the impact of attention on perceptual processing, we need to examine not just a localized brain region but how these regions interact with each other ([Saalman et al., 2007](https://www.ncbi.nlm.nih.gov/pubmed/17569863); [Gregoriou et al., 2009](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2849291/); [Al-Aidroos et al., 2012](https://www.ncbi.nlm.nih.gov/pubmed/22908274)). In this case, traditional MVPA techniques that examine patterns of activity in local brain regions will not be sufficient and a more global analysis will be helpful. The covarying response of two or more brain regions becomes critical in this case, and thus functional connectivity measures are important in studying these phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading <a id=\"load_data\"></a>\n",
    "\n",
    "Load the preprocessed data for subject 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import latatt_dir\n",
    "assert os.path.exists(latatt_dir)\n",
    "\n",
    "dir_time = os.path.join(latatt_dir, 'onsets', 'fsl')\n",
    "dir_motion = os.path.join(latatt_dir, 'processed_data', 'motionnuisance')\n",
    "dir_motion_background = os.path.join(latatt_dir, 'processed_data', 'background')\n",
    "\n",
    "sub = 'sub01'\n",
    "num_runs = 1\n",
    "TR = 1.5 # Duration of 1 TR in seconds\n",
    "scan_duration = 540 # Duration of the scan in seconds\n",
    "\n",
    "# Shift the data a certain amount\n",
    "shift_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-Study:** Explore the data\n",
    "\n",
    "**Exercise 1**:<a id=\"ex1\"></a> Carefully read the README file in `latatt_dir`. What is different between bg_image and raw_hires?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_file=os.path.join(latatt_dir,'README.txt')\n",
    "!head -100 $header_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**  \n",
    "\n",
    "raw_hires is the set of the raw hi res images belonging to each subject\n",
    "\n",
    "bg_image is the standard space average of the hi res anatomical images of all subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create stimulus labels and time-shift <a id=\"time_shift\"></a>\n",
    "\n",
    "From the stimulus timing files, we need to create labels for each TR in the BOLD data. We did this exercise for the VDC dataset in notebook 02-data-handling. You can repeat those steps to create labels for every TR.  \n",
    "\n",
    "As we have FSL onset files for this dataset, there is an alternative way to create labels for every TR using `fmrisim` in BrainIAK. This involves calling `generate_stimfunction` and then `shift_timing`, which we defined in `utils`.\n",
    "\n",
    "**Self-study:** What does `generate_stimfunction` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "<strong> To time-shift or not?</strong> In traditional MVPA analysis, we timeshift the BOLD signal to account for the hemodynamic lag. In functional connectivity analysis, time-shifting is sometimes not done to ensure that the voxels being compared are similar before, during, and after the stimulus has been shown. If you do not want to time-shift your data then set the `shift_size = 0`.\n",
    "\n",
    "<strong>Note:</strong> In this notebook, we are setting `shift_size=2`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the utilities from the simulator to create an event time course based on an FSL onset file\n",
    "\n",
    "# Rightward attention blocks.\n",
    "right_stimfunction = sim.generate_stimfunction(\n",
    "    onsets='', \n",
    "    event_durations='', \n",
    "    total_time=scan_duration,\n",
    "    temporal_resolution=1/TR, \n",
    "    timing_file=(dir_time + f'/{sub}/right.txt')\n",
    ")\n",
    "\n",
    "# Leftward attention blocks.\n",
    "left_stimfunction  = sim.generate_stimfunction(\n",
    "    onsets='', \n",
    "    event_durations='', \n",
    "    total_time=scan_duration,\n",
    "    temporal_resolution=1/TR, \n",
    "    timing_file=(dir_time + f'/{sub}/left.txt')\n",
    ")\n",
    "\n",
    "\n",
    "# Shift the timecourses to account for the hemodynamic lag.\n",
    "right_stim_lag = shift_timing(right_stimfunction, shift_size)\n",
    "left_stim_lag = shift_timing(left_stimfunction, shift_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Examine the header file<a id=\"header_info\"></a>\n",
    "\n",
    "We have been loading nifti files in all previous notebooks. There is useful information in the header that you should examine to learn about the data organization and shape. You can find the voxel size (mm) and the transform space of the data set. More information can be found [here.](http://nipy.org/nibabel/gettingstarted.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nifti object\n",
    "nii = nib.load(dir_motion + f'/{sub}.nii.gz')\n",
    "hdr=nii.header\n",
    "print(hdr)\n",
    "print(f'Voxel size in {hdr.get_xyzt_units()[0]}, time in {hdr.get_xyzt_units()[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Plot the stimulus labels<a id=\"stim_labels\"></a>\n",
    "\n",
    "In this experiment there are two conditions (attend left and attend right), and each one condition has a separate timing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the stim time course. \n",
    "f, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "ax.plot(right_stim_lag)\n",
    "ax.plot(left_stim_lag)\n",
    "ax.set_yticks([0,1])\n",
    "ax.set_xlabel('Timepoints')\n",
    "ax.set_ylabel('Stimulus')\n",
    "ax.legend(('Attend Right', 'Attend Left'), loc='upper right')\n",
    "ax.set_ylim(0, 1.3)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.  Mask and extract the whole-brain data <a id=\"mask\"></a>\n",
    "\n",
    "In previous notebooks, we extracted brain data with a mask. The mask was either an ROI or the whole brain. Both of these masks were already created for you. Here we show you a different way, using nilearn, to create a mask from a dataset and then extract the data from the mask. This function can z-score the data as well. More information can be found [here](http://nilearn.github.io/modules/generated/nilearn.input_data.NiftiMasker.html). We then plot the time course of a few voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a masker object that also standardizes the data\n",
    "masker_wb = input_data.NiftiMasker(\n",
    "    standardize=True,  # Are you going to zscore the data across time?\n",
    "    t_r=1.5, \n",
    "    memory='nilearn_cache',  # Caches the mask in the directory given as a string here so that it is easier to load and retrieve\n",
    "    memory_level=1,  # How much memory will you cache?\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Pull out the time course for voxel\n",
    "bold_wb = masker_wb.fit_transform(nii)\n",
    "bold_wb_r = bold_wb[(right_stim_lag==1),:]\n",
    "bold_wb_l = bold_wb[(left_stim_lag==1),:]\n",
    "\n",
    "print(f'whole brain bold time series shape: {np.shape(bold_wb)}')\n",
    "print(f'whole brain bold time series, attend left shape: {np.shape(bold_wb_l)}', )\n",
    "print(f'whole brain bold time series, attend right shape: {np.shape(bold_wb_r)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the timeseries for a few voxels\n",
    "\"\"\"\n",
    "voxel_ids = [0,10,100]\n",
    "\n",
    "f, ax = plt.subplots(2,1,figsize=(14, 8))\n",
    "ax[0].set_title(f'Voxel activity for rightward trials, voxel ids = {str(voxel_ids)}')\n",
    "ax[0].plot(bold_wb_r[:, voxel_ids])\n",
    "ax[0].legend(('voxel 0', 'voxel 10', 'voxel 100'))\n",
    "ax[0].set_ylabel('Evoked activity')\n",
    "ax[0].set_xlabel('')\n",
    "sns.despine()\n",
    "ax[1].set_title(f'Voxel activity for leftward trials, voxel ids = {str(voxel_ids)}')\n",
    "ax[1].plot(bold_wb_l[:, voxel_ids])\n",
    "ax[1].set_ylabel('Evoked activity')\n",
    "ax[1].set_xlabel('Timepoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a seed  <a id=\"seed\"></a>\n",
    "\n",
    "At this stage, we have loaded the whole-brain data for the attend-left and attend-right conditions. To examine the effects of attention, we are going to create seed ROIs and correlate their activity with other voxels in the brain. For any voxels that are correlated with a seed ROI, we can infer that they are functionally connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a spherical ROI  <a id=\"ROI_sphere\"></a>\n",
    "\n",
    "Let's create an ROI to define the parahippocampal place area (PPA), a scene selective region, which is likely important since scene stimuli were presented. The data are currently in MNI space, a standardized anatomical space that allows us to compare individual anatomy to a common, averaged space and use coordinates in MNI space to identify regions (approximately) in individual participants.\n",
    "\n",
    "**Exercise 2:**<a id=\"ex2\"></a> Use this article to determine the center coordinates for the left and right PPA (use the co-ordinates based on the MNI average, reported in the methods section): Park, S., & Chun, M. M. (2009). Different roles of the parahippocampal place area (PPA) and retrosplenial cortex (RSC) in panoramic scene perception. NeuroImage, 47(4), 1747â€“1756. https://doi.org/10.1016/j.neuroimage.2009.04.058. Note: they use asymmetric ROI coordinates for the left and right PPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the center of the left and right ROIs\n",
    "coords_lPPA = [( -27, -52, -8)]\n",
    "coords_rPPA = [(28, -47, -9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilearn has some powerful tools for drawing ROIs. These functions allow you to flexibly identify ROIs of any shape and have multiple parameters that allow for smoothing, detrending, filtering and standardization. However, it is easy to get things wrong with these functions so use these parameters cautiously. Most research labs have well-defined processing pipelines with these parameters set for multiple studies to avoid too much variation across studies. We will play it safe and use the most basic [sphere ROI](http://nilearn.github.io/modules/generated/nilearn.input_data.NiftiSpheresMasker.html) function from nilearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Plot the bold signal for a mask  <a id=\"mask_signal\"></a>\n",
    "\n",
    "The average bold signal for all voxels in the left PPA mask is computed and plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititialize the masking object for the left PPA.\n",
    "\n",
    "masker_lPPA = input_data.NiftiSpheresMasker(\n",
    "    coords_lPPA, \n",
    "    radius=8, standardize=True, t_r=TR,\n",
    "    memory='nilearn_cache', memory_level=1, verbose=0\n",
    ")\n",
    "\n",
    "# Mask the epi data and get a time series for the left PPA\n",
    "bold_lPPA = masker_lPPA.fit_transform(nii)\n",
    "\n",
    "# Plot the data from the seed region for both attention condition\n",
    "bold_lPPA_r = bold_lPPA[right_stim_lag == 1, :] # left PPA attend right\n",
    "bold_lPPA_l = bold_lPPA[left_stim_lag == 1, :] # left PPA attend left\n",
    "f, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.set_title('Left PPA ROI activity for right and left attention conditions')\n",
    "ax.plot(bold_lPPA_r)\n",
    "ax.plot(bold_lPPA_l)\n",
    "ax.legend(('Attend Right', 'Attend Left'))\n",
    "ax.set_ylabel('Evoked activity')\n",
    "ax.set_xlabel('Timepoints')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute the correlation matrix  <a id=\"correlation_matrix\"></a>\n",
    "\n",
    "We previously encountered correlation matrices in the `06-rsa` notebook on pattern similarity. Each cell in those matrices corresponded to the spatial correlation of the patterns of activity related to a pair of stimuli or tasks. We will again be using correlation matrices, but now for assessing temporal correlation or functional connectivity. Each cell in the functional connectivity matrix reflects the correlation of the BOLD timeseries between a pair of voxels or regions, and matrices can be calculated separately for each condition or even each trial.\n",
    "\n",
    "Below we go through a slow loop-based way of calculating the correlation of every voxel in the brain with the PPA seed region during the attend right condition. We will revisit optimized ways of calculating correlations in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate seed with every brain voxel. Loop through and extract data for every voxel.\n",
    "start_time = time.time()\n",
    "num_voxels = bold_wb_r.shape[1]\n",
    "all_corr = np.zeros((num_voxels, 1))\n",
    "\n",
    "bold_lPPA_r_vector = bold_lPPA_r.flatten() # vectorize\n",
    "# Loop through all voxels in the brain\n",
    "for v in range(num_voxels): \n",
    "    # Correlate each voxel in the brain with the PPA seed region during attend right\n",
    "    all_corr[v] = np.corrcoef(bold_lPPA_r_vector, bold_wb_r[:, v])[0, 1]\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Analysis duration for {num_voxels} voxels: {np.round(end_time - start_time,2)}s')\n",
    "\n",
    "print(f\"Seed-based correlation transformed: min = {np.round(all_corr.min(),3)}; max = {np.round(all_corr.max(),3)}\")\n",
    "\n",
    "# A histogram is always a useful first way of looking at your data.\n",
    "f, ax = plt.subplots(1,1)\n",
    "ax.hist(all_corr)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('Pearson correlation')\n",
    "ax.set_title('Left PPA, attend right condition')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be calculating seed-based correlations throughout this notebook, so let's make a function. It is also common to transform the correlations to a [Fisher-Z score](https://en.wikipedia.org/wiki/Fisher_transformation), as the bounded nature of Pearson correlation violates certain statistical assumptions. This generally does not have a big effect on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_correlation(wbBold, seedBold):\n",
    "    \n",
    "    \"\"\"Compute the correlation between a seed voxel vs. other voxels \n",
    "    Parameters\n",
    "    ----------\n",
    "    wbBold [2d array], n_stimuli x n_voxels \n",
    "    seedBold [2d array] n_stimuli x 1\n",
    "\n",
    "    Return\n",
    "    ----------    \n",
    "    seed_corr [2d array], n_stimuli x 1\n",
    "    seed_corr_fishZ [2d array], n_stimuli x 1\n",
    "    \"\"\"\n",
    "    \n",
    "    num_voxels = wbBold.shape[1]\n",
    "    seed_corr = np.zeros((num_voxels, 1))\n",
    "    \n",
    "    for v in range(num_voxels):    \n",
    "        seed_corr[v] = np.corrcoef(seedBold.flatten(), wbBold[:, v])[0, 1]\n",
    "        \n",
    "    # Transfrom the correlation values to Fisher z-scores    \n",
    "    seed_corr_fishZ = np.arctanh(seed_corr)\n",
    "    \n",
    "    return seed_corr, seed_corr_fishZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the function and print out the range of results.\n",
    "corr_lPPA_r, corr_fz_lPPA_r = seed_correlation(bold_wb_r, bold_lPPA_r)\n",
    "print(f\"Seed-based correlation Fisher-Z transformed: min = {np.round(corr_fz_lPPA_r.min(),3)}; max = {np.round(corr_fz_lPPA_r.max(),3)}\")\n",
    "\n",
    "# A histogram is always a useful first way of looking at your data.\n",
    "f, ax = plt.subplots(1,1)\n",
    "ax.hist(corr_fz_lPPA_r)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('Fisher-Z score')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can tranform the correlation array back to a Nifti image object that we can save\n",
    "img_corr_lPPA_r = masker_wb.inverse_transform(corr_fz_lPPA_r.T)\n",
    "img_corr_lPPA_r.to_filename('seed_rtstim.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plot the seed correlations  <a id=\"plot_seed\"></a>\n",
    "\n",
    "We plot the seed correlation with every other voxel. For better visualization, we set a threshold, showing only voxels above the threshold. Typically, thresholds are chosen based on statistical significance. We have chosen the threshold arbitrarily below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also visualize the correlation of the seed with every voxel\n",
    "threshold = .8\n",
    "\n",
    "# Nilearn has useful tools for plotting our results as a map\n",
    "r_map_ar = plotting.plot_stat_map(\n",
    "    img_corr_lPPA_r, \n",
    "    threshold=threshold,\n",
    "    cut_coords=coords_lPPA[0],\n",
    ")\n",
    "# Add the seed\n",
    "r_map_ar.add_markers(\n",
    "    marker_coords=coords_lPPA, \n",
    "    marker_color='g',\n",
    "    marker_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a different way to plot the same information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a glass brain\n",
    "plotting.plot_glass_brain(\n",
    "    img_corr_lPPA_r, \n",
    "    threshold=threshold,\n",
    "    colorbar=True, \n",
    "    plot_abs=False,\n",
    "    display_mode='lyrz', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**<a id=\"ex3\"></a> Compute and plot the correlation with the left PPA when participants are attending to the scene in the left visual field. Threshold this plot at 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# Correlate seed with every brain voxel. Loop through and extract data for every voxel.\n",
    "corr_lPPA_l, corr_fz_lPPA_l = seed_correlation(bold_wb_l, bold_lPPA_l)\n",
    "\n",
    "img_corr_lPPA_l = masker_wb.inverse_transform(corr_fz_lPPA_l.T)\n",
    "\n",
    "# Let's also visualize the correlation of the seed with every voxel\n",
    "threshold = .7\n",
    "\n",
    "# Nilearn has useful tools for plotting our results as a map\n",
    "l_map_ar = plotting.plot_stat_map(\n",
    "    img_corr_lPPA_l, \n",
    "    threshold=threshold,\n",
    "    cut_coords=coords_lPPA[0],\n",
    ")\n",
    "# Add the seed\n",
    "l_map_ar.add_markers(\n",
    "    marker_coords=coords_lPPA, \n",
    "    marker_color='g',\n",
    "    marker_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The early stages of visual processing in the brain occur in the contralateral side. If you are viewing something in your left visual field, your right visual brain areas will show greater activity. See if you notice something similar in your results from this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 4:**<a id=\"ex4\"></a> Create a spherical ROI corresponding to the right PPA (using the coordinates you created earlier and a radius of 10). Compute correlations across the whole-brain in each of the 'attend left' and 'attend right' conditions and plot your results.   \n",
    "_NB: Use the same plotting parameters (i.e., threshold, cut coordinates, colorbar) for each plot._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "masker_rPPA = input_data.NiftiSpheresMasker(\n",
    "    coords_rPPA, \n",
    "    radius=10, standardize=True, t_r=TR,\n",
    "    memory='nilearn_cache', memory_level=1, verbose=0\n",
    ")\n",
    "\n",
    "# Mask the epi data and get a time series for the left PPA\n",
    "bold_rPPA = masker_rPPA.fit_transform(nii)\n",
    "\n",
    "# Plot the data from the seed region for both attention condition\n",
    "bold_rPPA_r = bold_lPPA[right_stim_lag == 1, :] # right PPA attend right\n",
    "bold_rPPA_l = bold_lPPA[left_stim_lag == 1, :] # right PPA attend left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_rPPA_l, corr_fz_rPPA_l = seed_correlation(bold_wb_l, bold_rPPA_l)\n",
    "\n",
    "img_corr_rPPA_l = masker_wb.inverse_transform(corr_fz_rPPA_l.T)\n",
    "\n",
    "# Let's also visualize the correlation of the seed with every voxel\n",
    "threshold = .7\n",
    "\n",
    "# Nilearn has useful tools for plotting our results as a map\n",
    "r_map_ar = plotting.plot_stat_map(\n",
    "    img_corr_rPPA_l, \n",
    "    threshold=threshold,\n",
    "    cut_coords=coords_rPPA[0],\n",
    ")\n",
    "# Add the seed\n",
    "r_map_ar.add_markers(\n",
    "    marker_coords=coords_rPPA, \n",
    "    marker_color='g',\n",
    "    marker_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_rPPA_r, corr_fz_rPPA_r = seed_correlation(bold_wb_r, bold_rPPA_r)\n",
    "\n",
    "img_corr_rPPA_r = masker_wb.inverse_transform(corr_fz_rPPA_r.T)\n",
    "\n",
    "# Let's also visualize the correlation of the seed with every voxel\n",
    "threshold = .7\n",
    "\n",
    "# Nilearn has useful tools for plotting our results as a map\n",
    "r_map_ar = plotting.plot_stat_map(\n",
    "    img_corr_rPPA_r, \n",
    "    threshold=threshold,\n",
    "    cut_coords=coords_rPPA[0],\n",
    ")\n",
    "# Add the seed\n",
    "r_map_ar.add_markers(\n",
    "    marker_coords=coords_rPPA, \n",
    "    marker_color='g',\n",
    "    marker_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Creating a seed from an atlas  <a id=\"ROI_atlas\"></a>\n",
    "\n",
    "In addition to creating our own seed ROIs, we can use available atlases to extract ROIs.  Nilearn provides an easy way to accomplish this. In this next section you will:\n",
    "\n",
    "1. Use Nilearn to import an [atlas parcellation](http://nilearn.github.io/modules/reference.html#module-nilearn.datasets). This will download and save in your home directory by default. \n",
    "2. Explore the atlas: plot the different parcels and get their labels.\n",
    "3. Choose one parcel as the seed and extract the average BOLD signal over time from voxels in the parcel.\n",
    "4. Perform correlation with remaining parcels or voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "atlas_filename = atlas.maps\n",
    "\n",
    "# This is where the atlas is saved.\n",
    "print(f\"Atlas path: {atlas_filename}\")\n",
    "\n",
    "# Plot the ROIs\n",
    "plotting.plot_roi(atlas_filename)\n",
    "print('Harvard-Oxford cortical atlas')\n",
    "\n",
    "# Print the labels\n",
    "# Label 0 (Background) refers to the brain image, not background connectivity\n",
    "\n",
    "# Create a Pandas dataframe of the atlas data for easy inspection.\n",
    "atlas_df = pd.DataFrame(atlas)\n",
    "atlas_df['ROI_ID'] = np.arange(1,len(atlas_df)+1)\n",
    "atlas_df.head()\n",
    "\n",
    "# Create a masker object that we can use to select ROIs\n",
    "atlas_masker = NiftiLabelsMasker(labels_img=atlas_filename)\n",
    "print(atlas_masker.get_params())\n",
    "\n",
    "# Apply the Harvard-Oxford atlas to the Nifti object so we can pull out data from single parcels/ROIs\n",
    "bold_HarOxf = atlas_masker.fit_transform(nii)\n",
    "print(f'Shape of parcellated bold time courses: {np.shape(bold_HarOxf)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: What do the dimensions of `bold_HarOxf` mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bold_HarOxf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**A**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous analyses we calculated the average activity in a single seed region across time. However, Nilearn has tools to easily calculate the timecourse of activity across all of the ROIs that are supplied to the masker object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for rightward attention only\n",
    "bold_HarOxf_r = bold_HarOxf[(right_stim_lag==1),:]\n",
    "\n",
    "# What does our data structure look like?\n",
    "print(f\"All time points {bold_HarOxf.shape}\")\n",
    "print(f\"Attend right trials: {bold_HarOxf_r.shape}\")\n",
    "\n",
    "# Pull out a single ROI corresponding to the posterior parahippocampal cortex\n",
    "# Parahippocampal Gyrus, posterior division. \n",
    "\n",
    "roi_id = atlas_df[atlas_df['labels'] == \"Parahippocampal Gyrus, posterior division\"]['ROI_ID'] -1 # account for zero-based indexing\n",
    "bold_HarOxf_pPHG_r = np.array(bold_HarOxf_r[:, roi_id])\n",
    "print(bold_HarOxf_pPHG_r.shape)\n",
    "print(f\"Posterior parahippocampal gyrus (region 35) rightward attention trials shape: {bold_HarOxf_pPHG_r.shape}\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(14,4))\n",
    "ax.plot(bold_HarOxf_pPHG_r)\n",
    "ax.set_ylabel('Evoked activity')\n",
    "ax.set_xlabel('Timepoints')\n",
    "ax.set_title('Posterior parahippocampal gyrus timeseries, rightward attention trials')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like before, let's correlate the whole-brain timeseries with the seed we have pulled out\n",
    "\n",
    "corr_pPHG_r, corr_fz_pPHG_r = seed_correlation(bold_wb_r, bold_HarOxf_pPHG_r) \n",
    "\n",
    "# Print the range of correlations.\n",
    "print(f\"PHG correlation Fisher-z transformed: min = {np.round(corr_fz_pPHG_r.min(), 3)}; max = {np.round(corr_fz_pPHG_r.max(), 3)}\")\n",
    "\n",
    "# Plot a histogram\n",
    "f, ax = plt.subplots()\n",
    "ax.hist(corr_fz_pPHG_r)\n",
    "ax.set_ylabel('Frequency');\n",
    "ax.set_xlabel('Fisher-z score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map back to the whole brain image\n",
    "img_corr_pPHG_r = masker_wb.inverse_transform(\n",
    "    corr_fz_pPHG_r.T\n",
    ")\n",
    "\n",
    "threshold = .8 \n",
    "\n",
    "# Find the cut coordinates of this ROI, using parcellation.\n",
    "# This function takes the atlas path and the hemisphere and outputs all centers of the ROIs\n",
    "roi_coords = plotting.find_parcellation_cut_coords(atlas_filename,label_hemisphere='right')\n",
    "\n",
    "# Pull out the coordinate for this ROI\n",
    "roi_coord = roi_coords[roi_id, :]\n",
    "\n",
    "# Plot the correlation as a map on a standard brain. \n",
    "results_pHG_r = plotting.plot_stat_map(\n",
    "    img_corr_pPHG_r, \n",
    "    threshold=threshold,\n",
    "    cut_coords=np.squeeze(roi_coord),\n",
    ")\n",
    "results_pHG_r.add_markers(\n",
    "    marker_coords=roi_coord, \n",
    "    marker_color='b',\n",
    "    marker_size=50\n",
    ")\n",
    "\n",
    "# Create a glass brain\n",
    "plotting.plot_glass_brain(\n",
    "    img_corr_pPHG_r, \n",
    "    threshold=threshold,\n",
    "    colorbar=True, \n",
    "    display_mode='lyrz', \n",
    "    plot_abs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Compute connectivity across parcels <a id=\"parcel_corr\"></a>\n",
    "\n",
    "In addition to one ROI, we can compute correlations across multiple brain regions. Nilearn has a [function](http://nilearn.github.io/modules/reference.html#module-nilearn.connectome) to do this quite easily. This will be useful when we want to study attention in different brain regions.\n",
    "\n",
    "We will also plot the connectivity matrices using a few different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the connectivity object.\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "# Calculate the correlation of each parcel with every other parcel during attend-right condition\n",
    "corr_mat_HarOxf_r = correlation_measure.fit_transform([bold_HarOxf_r])[0]\n",
    "\n",
    "# Remove the diagonal for visualization (guaranteed to be 1.0)\n",
    "np.fill_diagonal(corr_mat_HarOxf_r, np.nan)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "# The labels of the Harvard-Oxford Cortical Atlas that we are using \n",
    "# start with the background, hence we skip the first label\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "im=ax.imshow(corr_mat_HarOxf_r, interpolation='None', cmap='RdYlBu_r')\n",
    "ax.set_yticks(range(len(atlas.labels[1:])))\n",
    "ax.set_yticklabels(atlas.labels[1:], fontsize=4)\n",
    "ax.set_xticks(range(len(atlas.labels[1:])))\n",
    "ax.set_xticklabels(atlas.labels[1:], rotation=90, fontsize=4)\n",
    "ax.set_title('Parcellation correlation matrix')\n",
    "f.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternatively, we could use Nilearn's own plotting function\n",
    "plotting.plot_matrix(\n",
    "    corr_mat_HarOxf_r, \n",
    "    cmap='RdYlBu_r', \n",
    "    figure=(6,6), \n",
    "    auto_fit=False,\n",
    "    labels=atlas.labels[1:],\n",
    "    colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:**<a id=\"ex6\"></a> We mentioned earlier that parietal and frontal cortices modulates sensory processing. You are now ready to examine this. Use a parcel from the **Middle Frontal Gyrus** as a seed and compute the **voxel-wise** correlation across the brain for the attend left and attend right conditions. \n",
    "\n",
    "When plotting the data, use `plotting.find_parcellation_cut_coords` to find the center of each ROI. Then use these coordinates to specify the `cut_coords` to center your plot on. Threshold your plots at 0.65.\n",
    "\n",
    "Also note that roi_id does not align with the ROI names in the pandas dataframe above. Specifically, each roi_id is 1 less than the number reported in this table. That is because an ROI is not made for the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "\n",
    "# Get data for rightward attention only\n",
    "bold_HarOxf_r = bold_HarOxf[(right_stim_lag==1),:]\n",
    "\n",
    "# What does our data structure look like?\n",
    "print(f\"All time points {bold_HarOxf.shape}\")\n",
    "print(f\"Attend right trials: {bold_HarOxf_r.shape}\")\n",
    "\n",
    "# Pull out a single ROI corresponding to the posterior parahippocampal cortex\n",
    "# Parahippocampal Gyrus, posterior division. \n",
    "\n",
    "roi_id = atlas_df[atlas_df['labels'] == \"Middle Frontal Gyrus\"]['ROI_ID'] -1 # account for zero-based indexing\n",
    "bold_HarOxf_pMFG_r = np.array(bold_HarOxf_r[:, roi_id])\n",
    "print(bold_HarOxf_pMFG_r.shape)\n",
    "print(f\"Middle Frontal Gyrus rightward attention trials shape: {bold_HarOxf_pMFG_r.shape}\")\n",
    "\n",
    "corr_pMFG_r, corr_fz_pMFG_r = seed_correlation(bold_wb_r, bold_HarOxf_pMFG_r) \n",
    "\n",
    "print(np.shape(corr_fz_pMFG_r))\n",
    "img_corr_pMFG_r = masker_wb.inverse_transform(\n",
    "    corr_fz_pMFG_r.T\n",
    ")\n",
    "\n",
    "threshold = .8 \n",
    "\n",
    "# Find the cut coordinates of this ROI, using parcellation.\n",
    "# This function takes the atlas path and the hemisphere and outputs all centers of the ROIs\n",
    "roi_coords = plotting.find_parcellation_cut_coords(atlas_filename,label_hemisphere='right')\n",
    "\n",
    "# Pull out the coordinate for this ROI\n",
    "roi_coord = roi_coords[roi_id, :]\n",
    "\n",
    "# Plot the correlation as a map on a standard brain. \n",
    "results_pMFG_r = plotting.plot_stat_map(\n",
    "    img_corr_pMFG_r, \n",
    "    threshold=threshold,\n",
    "    cut_coords=np.squeeze(roi_coord),\n",
    ")\n",
    "results_pMFG_r.add_markers(\n",
    "    marker_coords=roi_coord, \n",
    "    marker_color='b',\n",
    "    marker_size=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Background connectivity <a id=\"back_conn\"></a>\n",
    "\n",
    "There is a potential problem in analyzing functional connectivity during tasks. Consider brain regions A and B. Let's assume that our stimuli activate both regions. If we were to examine correlations between the regions, they would have strong connectivity, but not because they are necessarily communicating or interacting in any way. Rather, they share the stimulus as a third variable. One solution is to regress out the stimulus-evoked responses from our signal and re-examine the correlations between regions. If region A and B are still correlated, we are on more solid footing that they are functionally connected during the task. Insofar as this \"background connectivity\" differs between task conditions (e.g., attend left vs. right), we can conclude that the task is modulating the scaffold of noise correlations in the brain. To learn more about background connectivity, see this [review](https://doi.org/10.1126/science.1238409).\n",
    "\n",
    "In background connectivity analysis, stimulus-driven activation is not the desired effect of interest, but potentially a confound. Thus, now we need to remove \"stimulus confounds\" before continuing. Lucky for us, the dataset in the directory `../processed_data/background/`  already has the evoked activity and other nuisance variables regressed out. We'll repeat the previous analyses for the left PPA on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "sub = 'sub01'\n",
    "epi_in_mcg = dir_motion_background + f'/{sub}.nii.gz'\n",
    "\n",
    "# Get the seed data\n",
    "bold_lPPA_mcg = masker_lPPA.fit_transform(epi_in_mcg)\n",
    "bold_lPPA_r_mcg = bold_lPPA_mcg[right_stim_lag==1,:]\n",
    "bold_lPPA_l_mcg = bold_lPPA_mcg[left_stim_lag==1,:]\n",
    "\n",
    "# Get the whole brain data\n",
    "bold_wb_mcg = masker_wb.fit_transform(epi_in_mcg)\n",
    "bold_wb_r_mcg = bold_wb_mcg[right_stim_lag==1,:] \n",
    "bold_wb_l_mcg = bold_wb_mcg[left_stim_lag==1,:] \n",
    "\n",
    "# plot the data\n",
    "f, ax = plt.subplots(figsize=(14,4))\n",
    "plt.plot(bold_lPPA_r_mcg)\n",
    "plt.plot(bold_lPPA_l_mcg)\n",
    "ax.legend(('Attend Right', 'Attend Left'))\n",
    "ax.set_ylabel('BOLD signal, normalized')\n",
    "ax.set_xlabel('TRs of right attention blocks')\n",
    "ax.set_title('Background activity in seed region')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the voxelwise seed-based correlation\n",
    "corr_lPPA_r_mcg, corr_fz_lPPA_r_mcg  = seed_correlation(bold_wb_r_mcg, bold_lPPA_r_mcg)\n",
    "corr_lPPA_l_mcg, corr_fz_lPPA_l_mcg  = seed_correlation(bold_wb_l_mcg, bold_lPPA_l_mcg)\n",
    "\n",
    "# Make an image \n",
    "img_corr_fz_lPPA_r_mcg = masker_wb.inverse_transform(corr_fz_lPPA_r_mcg.T)\n",
    "img_corr_fz_lPPA_l_mcg = masker_wb.inverse_transform(corr_fz_lPPA_l_mcg.T)\n",
    "\n",
    "print('lPPA correlation Fisher-z transformed')\n",
    "print(\"Attend right: min = %.3f; max = %.3f\" % (\n",
    "    corr_fz_lPPA_r_mcg.min(), corr_fz_lPPA_r_mcg.max()))\n",
    "print(\"Attend left: min = %.3f; max = %.3f\" % (\n",
    "    corr_fz_lPPA_l_mcg.min(), corr_fz_lPPA_l_mcg.max()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation of each voxel in the brain with the seed\n",
    "threshold = .8\n",
    "vmax = np.max(np.stack([corr_fz_lPPA_r_mcg, corr_fz_lPPA_l_mcg]))\n",
    "\n",
    "f, ax = plt.subplots(2, 2, figsize = (16, 7))\n",
    "\n",
    "# Attend left\n",
    "ax[0,0].set_title('Attend left, lPPA')\n",
    "\n",
    "r_map = plotting.plot_stat_map(\n",
    "    img_corr_fz_lPPA_l_mcg, \n",
    "    threshold=threshold, vmax=vmax, \n",
    "    cut_coords=coords_lPPA[0], \n",
    "    axes=ax[0,0]\n",
    ")\n",
    "r_map.add_markers(\n",
    "    marker_coords=coords_lPPA, \n",
    "    marker_color='g',\n",
    "    marker_size=50\n",
    ")\n",
    "plotting.plot_glass_brain(\n",
    "    img_corr_fz_lPPA_l_mcg, \n",
    "    threshold=threshold, vmax=vmax, \n",
    "    colorbar=True, \n",
    "    display_mode='lyrz', \n",
    "    plot_abs=False, \n",
    "    axes=ax[1,0]\n",
    ")\n",
    "\n",
    "# Attend right\n",
    "ax[0,1].set_title('Attend right, lPPA')\n",
    "r_map = plotting.plot_stat_map(\n",
    "    img_corr_fz_lPPA_r_mcg, \n",
    "    threshold=threshold, vmax=vmax, \n",
    "    cut_coords=coords_lPPA[0], \n",
    "    axes=ax[0,1]\n",
    ")\n",
    "r_map.add_markers(\n",
    "    marker_coords=coords_lPPA, \n",
    "    marker_color='g',\n",
    "    marker_size=50\n",
    ")\n",
    "plotting.plot_glass_brain(\n",
    "    img_corr_fz_lPPA_r_mcg, \n",
    "    threshold=threshold, vmax=vmax, \n",
    "    colorbar=True, \n",
    "    display_mode='lyrz', \n",
    "    plot_abs=False, \n",
    "    axes=ax[1,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:**<a id=\"ex7\"></a> Compare background connectivity (above) to the original left PPA spherical ROI result (from [3.1](#plot_seed)). Explain why this difference might exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**   \n",
    "This one has much lower correlation and sometimes even negative correlation coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a correlation matrix with the background connectivity data for one condition (attend right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcellate the time course to get the background connectivity parcels\n",
    "bold_HarOxf_timeseries_mcg = atlas_masker.fit_transform(epi_in_mcg)\n",
    "bold_HarOxf_timeseries_mcg_r = bold_HarOxf_timeseries_mcg[right_stim_lag==1, :]\n",
    "\n",
    "correlation_matrix_mcg = correlation_measure.fit_transform([bold_HarOxf_timeseries_mcg_r])[0]\n",
    "\n",
    "# Remove the diagonal for visualization (guaranteed to be 1.0)\n",
    "np.fill_diagonal(correlation_matrix_mcg, np.nan)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "f, ax =plt.subplots(figsize=(6,6))\n",
    "\n",
    "# The labels we have start with the background (0), hence we skip the first label\n",
    "ax.set_title('Background connnectivity correlation for parcellation')\n",
    "im=ax.imshow(correlation_matrix_mcg, interpolation='None', cmap='RdYlBu_r')\n",
    "ax.set_yticks(range(len(atlas.labels)-1))\n",
    "ax.set_yticklabels(atlas.labels[1:],fontsize=4)\n",
    "ax.set_xticks(range(len(atlas.labels)-1))\n",
    "ax.set_xticklabels(atlas.labels[1:], rotation=90,fontsize=4)\n",
    "f.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:**<a id=\"ex7\"></a> Use a different atlas (use [`datasets.fetch_atlas_`](http://nilearn.github.io/modules/reference.html#module-nilearn.datasets) to look through other available atlases) and recompute the background connectivity matrix for both attend left and attend right conditions. Use an atlas that distinguishes ROIs between the left and right hemispheres and rearrange the labels so that all the ROIs from a hemisphere are grouped. What structure do you notice in the correlation matrix if any and what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "sub = 'sub01'\n",
    "Jepi = dir_motion_background + f'/{sub}.nii.gz'\n",
    "\n",
    "# peter recommended the Juelich Atlas\n",
    "\n",
    "Jatlas = datasets.fetch_atlas_juelich('maxprob-thr25-2mm')\n",
    "Jatlas_filename = Jatlas.maps\n",
    "\n",
    "# Create a Pandas dataframe of the atlas data for easy inspection.\n",
    "Jatlas_df = pd.DataFrame(Jatlas)\n",
    "Jatlas_df['ROI_ID'] = np.arange(1,len(Jatlas_df)+1)\n",
    "\n",
    "\n",
    "# Create a masker object that we can use to select ROIs\n",
    "Jatlas_masker = NiftiLabelsMasker(labels_img=Jatlas_filename)\n",
    "\n",
    "# Apply the J atlas to the Nifti object so we can pull out data from single parcels/ROIs\n",
    "bold_J = Jatlas_masker.fit_transform(Jepi)\n",
    "\n",
    "bold_J_l = bold_J[left_stim_lag==1, :]\n",
    "bold_J_r = bold_J[right_stim_lag==1, :]\n",
    "\n",
    "correlation = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "Jcorrelation_matrix_l = correlation_measure.fit_transform([bold_J_l])[0]\n",
    "Jcorrelation_matrix_r = correlation_measure.fit_transform([bold_J_r])[0]\n",
    "\n",
    "# Remove the diagonal for visualization (guaranteed to be 1.0)\n",
    "np.fill_diagonal(Jcorrelation_matrix_l, np.nan)\n",
    "np.fill_diagonal(Jcorrelation_matrix_r, np.nan)\n",
    "\n",
    "\n",
    "# Plot the correlation matrix\n",
    "f, ax =plt.subplots(figsize=(6,6))\n",
    "\n",
    "# The labels we have start with the background (0), hence we skip the first label\n",
    "ax.set_title('Background connnectivity correlation for parcellation')\n",
    "im=ax.imshow(Jcorrelation_matrix_l, interpolation='None', cmap='RdYlBu_r', vmin = -1, vmax = 1)\n",
    "ax.set_yticks(range(len(Jatlas.labels)-1))\n",
    "ax.set_yticklabels(Jatlas.labels[1:],fontsize=4)\n",
    "ax.set_xticks(range(len(Jatlas.labels)-1))\n",
    "ax.set_xticklabels(Jatlas.labels[1:], rotation=90,fontsize=4)\n",
    "f.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "f, ax =plt.subplots(figsize=(6,6))\n",
    "\n",
    "# The labels we have start with the background (0), hence we skip the first label\n",
    "ax.set_title('Background connnectivity correlation for parcellation')\n",
    "im=ax.imshow(Jcorrelation_matrix_r, interpolation='None', cmap='RdYlBu_r', vmin = -1, vmax = 1)\n",
    "ax.set_yticks(range(len(Jatlas.labels)-1))\n",
    "ax.set_yticklabels(Jatlas.labels[1:],fontsize=4)\n",
    "ax.set_xticks(range(len(Jatlas.labels)-1))\n",
    "ax.set_xticklabels(Jatlas.labels[1:], rotation=90,fontsize=4)\n",
    "f.colorbar(im,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Group analyses <a id=\"group\"></a>\n",
    "\n",
    "\n",
    "**Exercise 9:**<a id=\"ex9\"></a> Calculate and store the two matrices computed in [Exercise 8](#ex8) (functional connectivity separately for each attention condition) for all 30 participants using the **Harvard-Oxford Atlas**. Plot the average across participants for each condition.\n",
    "\n",
    "Here are some suggested concrete steps to help you break down this task. We recommend you make at least the following functions to encapsulate some of these steps.\n",
    "\n",
    "1. Create a function `get_data_timing` that takes `dir_time`, `sub`, `TR`, `scan_duration`, and `shift_size`, generates the stimulus functions for the attend left and attend right conditions, shifts the timing of these functions, and returns them.\n",
    "\n",
    "2. Create a function `get_atlas_bold_data_labeled` that takes `dir_motion_background`, `sub`, `atlas_filename`, `stim_labels_l`, and `stim_labels_r`, and returns two arrays -- the masked bold data for the attend left and attend right conditions. \n",
    "\n",
    "3. Create a function `compute_connectivity_matrix` that takes the masked bold data for a single condition (from `get_atlas_bold_data_labeled`) and returns a parcel-by-parcel connectivity matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert card here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:**<a id=\"ex10\"></a> Taking the data from Ex 9, calculate the difference between the attend left and attend right correlation matrix for each participant (if they are stacked matrices, then you only need to subtract the 3d arrays). You should now have 30 difference matrices, one per subject. This allows you to conduct a simple statistical test of how reliably left vs. right attention affects background connectivity between parcels in the sample. We will use a one-sample t-test for this purpose (`stats.ttest_1samp`), which takes in the difference matrix for each participant and the `popmean`. The `popmean` for this test is 0, representing the value we are trying to be different from. \n",
    "\n",
    "Plot two matrices: the t-statistics of the pairwise differences and the p-values of these t-statistics. Threshold your p-value plot at a meaningful level (p = 0.05, uncorrected).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11:**<a id=\"ex11\"></a>  Looking at your p-values from exercise 10, how many connections significantly (p < 0.05, uncorrected) differentiate between the attend-left and attend-right conditions? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting a connectome** <a id=\"connectome\"></a>\n",
    "\n",
    "Nilearn has some beautiful tools for plotting connectomes. [`plotting.plot_connectome`](http://nilearn.github.io/modules/generated/nilearn.plotting.plot_connectome.html) takes in the node by node correlation matrix and a node by coordinate matrix and then creates a connectome. Thresholds can be used to only show strong connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the atlas.\n",
    "atlas_filename = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm') \n",
    "atlas_nii = atlas_filename.maps\n",
    "atlas_data = atlas_filename.maps.get_fdata()\n",
    "labels = np.unique(atlas_data)\n",
    "\n",
    "# Iterate through all of the ROIs.\n",
    "coords = []\n",
    "for label_id in labels:\n",
    "    \n",
    "    # Skip the background.\n",
    "    if label_id == 0:\n",
    "        continue\n",
    "        \n",
    "    # Pull out the ROI of within the mask.    \n",
    "    roi_mask = (atlas_data == label_id)\n",
    "    \n",
    "    # Create as a nifti object so it can be read by the cut coords algorithm.\n",
    "    nii = nib.Nifti1Image(roi_mask.astype('int16'), atlas_nii.affine)\n",
    "    \n",
    "    # Find the center of mass of the connectome.\n",
    "    coords.append(plotting.find_xyz_cut_coords(nii))\n",
    "    \n",
    "# Plot the connectome.\n",
    "plotting.plot_connectome(correlation_matrix_mcg, coords, edge_threshold='95%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:** <a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "# Load the atlas.\n",
    "atlas_filename = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm') \n",
    "atlas_nii = atlas_filename.maps\n",
    "atlas_data = atlas_filename.maps.get_fdata()\n",
    "labels = np.unique(atlas_data)\n",
    "\n",
    "# Iterate through all of the ROIs.\n",
    "coords = []\n",
    "for label_id in labels:\n",
    "    \n",
    "    # Skip the background.\n",
    "    if label_id == 0:\n",
    "        continue\n",
    "        \n",
    "    # Pull out the ROI of within the mask.    \n",
    "    roi_mask = (atlas_data == label_id)\n",
    "    \n",
    "    # Create as a nifti object so it can be read by the cut coords algorithm.\n",
    "    nii = nib.Nifti1Image(roi_mask.astype('int16'), atlas_nii.affine)\n",
    "    \n",
    "    # Find the center of mass of the connectome.\n",
    "    coords.append(plotting.find_xyz_cut_coords(nii))\n",
    "    \n",
    "# Plot the connectome.\n",
    "plotting.plot_connectome(correlation_matrix_mcg, coords, edge_threshold='95%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a>\n",
    "\n",
    "B. Hutchinson provided data and provided initial code  \n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 3/15/18  \n",
    "Q. Lu add solution   \n",
    "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  \n",
    "C. Ellis implemented updates from cmhn-s19<br/>\n",
    "X. Li improved figures in section 1.3 and 1.4      \n",
    "T. Yates made edits for cmhn_s21  \n",
    "E. Busch made edits for cmhn_s22, cmhn_s23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
