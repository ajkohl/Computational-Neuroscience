{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bc423121-dc96-4712-829b-43480027e1f3"
    },
    "tags": []
   },
   "source": [
    "# Sherlock Mystery\n",
    "\n",
    "Solve a series of mysteries as described below. To solve these you must integrate all of the skills you have learned in this class.\n",
    "\n",
    "If you decide to make a separate notebook or python script(s) for each question, then name them with the appropriate prefix (e.g., q1). You may find it useful to write a utilities file with some common functions or variables you may want to reuse throughout the assignment, as we have done with the `utils.py` file released with each assignment. You should then commit these files along with this notebook to document your answers. You may also want to save intermediate data throughout this process; feel free to do so and note where you do this in your code.\n",
    "\n",
    "You must answer all 4 questions. Your score out of 30 points will depend on the following criteria:\n",
    "\n",
    ">**Answer** (6 points, 2 points each for Q2, Q3, Q4): Did you get the right answer?  \n",
    ">**Approach** (16 points, 4 points for each question): Does the code reproduce your answer and was your approach sound, innovative, and efficient?  \n",
    ">**Clarity** (8 points, 2 points for each question): Is the code readable, clearly commented, and well-structured?\n",
    "\n",
    "**Due Date:** The final project is due back on **May 3 at 11:59pm**. Whatever is committed to your assignment repo at that time will be submitted automatically. If you complete the assignment early, please Slack us so we can begin grading. Final grades are due in some cases within 48 hours, so late submissions will not be accepted.\n",
    "\n",
    "You should think about your final submission as a \"tutorial\" for somebody learning these techniques. The quality and clarity of your code should meet the standard of being published publicly. Integrate all the best coding practices you have learned in this course in completing these assignments. \n",
    "\n",
    "Science is a collective enterprise and thus you are welcome to talk with your peers. However, you must write your own code, provide your own explanations and comments, and generate your final answers on your own.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<strong>Important:</strong> The dataset we are using has been published in several articles, meaning that there are ways to find or verify some of the answers by looking at published data and labels. Although we encourage you to read these papers, please only look at the data and labels provided on Grace. Any evidence of doing otherwise, including providing an answer without code that can re-generate it from the data/labels we provided, will result in a score of zero.\n",
    "</div>\n",
    "\n",
    "When committing files to github, make sure you do not commit any of the data or results you generate. These can get prohibitively large and make it hard for you to use git. To avoid this, **only upload notebooks and scripts**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Participants watched an episode of BBC's Sherlock in two parts during fMRI. You may watch the parts in the links below, but do not save, post, or share these copyrighted files:\n",
    "\n",
    "Part 1: https://1drv.ms/v/s!Aobi2ryypFQCgpZ4UTJfrzPgJBO1jw\n",
    "\n",
    "Part 2: https://1drv.ms/v/s!Aobi2ryypFQCgpoabssIiGezcNL1zA\n",
    "\n",
    "The length of the scans are (after 20 TRs were cropped from beginning):\n",
    "\n",
    "> Part 1: 946 TRs  \n",
    "> Part 2: 1030 TRs\n",
    "\n",
    "For the questions below, we will provide you with the full data (scans and labels) from Part 1 and partial data from Part 2.\n",
    "\n",
    "All of these data are stored here: `/gpfs/gibbs/project/cmhn/data/Sherlock_processed/`\n",
    "\n",
    "**Data:** Data for all subjects, part 1, are stored in the `part_1` directory. Partial data specific to questions 2, 3, & 4 are explained with their respective questions. \n",
    "\n",
    "**Masks:** We have provided the following masks that *may* be useful to you, stored in the `masks` directory:\n",
    "> `pmc_nn`: posterior medial cortex(functionally implicated in tasks as diverse as attention, memory, spatial navigation, emotion, self-relevance detection, and reward evaluation.)\n",
    "\n",
    "> `ddmn_mpfc`: medial pre-frontal cortex(attention, inhibitory control, habit formation and working, spatial or long-term memory)\n",
    "\n",
    "> `aud_early`: early auditory cortex  \n",
    "> `a1_rev`: the first auditory area  \n",
    "> `early_visual`: early visual cortex  \n",
    "\n",
    "> `intersect_mask`: whole-brain mask of voxels common to all subjects  \n",
    "> `MNI152_T1_3mm_brain_mask`: reference MNI mask\n",
    "\n",
    "**Regressor information:** The regressor_file.csv contains extremely useful and detailed information for describing and labeling *both* parts of the movie.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Important:</strong> These TRs have already been shifted to align the peak HRF with the labels (i.e., the label for time point *t* corresponds to TR *t* in the processed data).\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Classification\n",
    "\n",
    "Using the regressor file, build classifiers of movie content (e.g., is the scene indoor or outdoor, is Sherlock present on the screen, and many other options). Without double-dipping or overfitting, you should aim to get the highest classification accuracy possible, using the training sample of data (Part 1). It is up to you to choose how to use/group the labels, what mask to use, etc.\n",
    "\n",
    "In your answer, you should present three distinct classifiers, **each focusing on a different regressor**, that yield promising results (i.e., where you think the classifier is actually picking up some signal). **Importantly, this question will be graded purely for approach and clarity, not for accuracy or performance**. You should be motivated to come up with classifiers that are actually useful, because you will need accurate classifiers to decode the mystery segment in Question 3. You are welcome to include more than three classifiers in your answer, but if so, you should tell us which of the three you would like us to grade. \n",
    "\n",
    "Some important notes: \n",
    "\n",
    "1) Keep in mind that nearby TRs are highly correlated; as such, methods like StratifiedShuffleSplit that can assign adjacent TRs to training and test sets are effectively double-dipping, so you should avoid using these methods.\n",
    "\n",
    "2) If the labels are not balanced (within and across folds), this can bias results; for example, if 99% of the TRs have a label of 1 and 1% have a value of 0, then the classifier can achieve 99% accuracy simply by guessing “1” all of the time. For this reason, you should strive to have the labels be relatively balanced. However, in real-world data, this can be difficult or impossible to achieve; it is OK to run unbalanced analyses, but keep the unbalanced nature of the labels in mind when you are assessing how good performance is for a particular classifier, or you can create functions to balance the labels during training.\n",
    "\n",
    "3) Keep in mind that you can transform the regressors to improve balance. e.g., the “arousal” regressor has 5 levels, but you can binarize it so that it has two roughly-equally-balanced groups instead of 5 imbalanced groups.\n",
    "\n",
    "4) If you find a particularly good approach, we encourage you to share a verbal description of the approach on Slack so others can build on your idea. Just make sure that you only share a verbal description of the approach and not the actual code. And if you read about a cool approach on Slack, feel free to try variants of that approach for your own work - that is completely allowed.\n",
    "\n",
    "\n",
    "**Data directory:** `/gpfs/gibbs/project/cmhn/data/Sherlock_processed/part_1/`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize your approach to classification\n",
    "What three different regressors did you choose for your classifier? What kind of data did you use as input to the classifier? What classifier parameters did you use and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "  \n",
    "I chose the regressors indoor/outdoor scene, valence, and arousal as I assumed that there would be distinct masks which would be most useful in predicting each respectively. I performed an analysis where I used every available mask to predict every regressor and thus identified the masks that had the highest accuracy in predicting each regressor.  \n",
    "  \n",
    "the best mask for predicting Indoor/Outdoor Scenes is the visual mask with an accuracy of 0.637, with chance being 0.5\n",
    "\n",
    "the best mask for predicting Valence is tied between a1_maskedData, pmc_maskedData, and visual_maskedData all at accuracy = 0.865\n",
    "\n",
    "the best mask for predicting Arousal is the whole brain mask with an accuracy of 0.61, with chance being 0.5, however since the accuracy is low and this is a large mask that takes very long to run in a classifier, we can utilize the much smaller early auditory cortex (aud) mask for further optimization, which had an accuracy of 0.591.  \n",
    "  \n",
    "I used classifier parameters C = 0.01, gamma = 0.01, kernel = 'linear' for the aforementioned analyses and for the indoor/outdoor classifier and valence classifier as both of those had satisfactory above chance accuracy.\n",
    "\n",
    "I wanted to improve the accuracy of Arousal to be above 0.60 and there I utilized GridSearchCV and SelectKBest to determine that selecting for the k = 280 best features with the classifier parameters set to C = 0.1, gamma = 0.01, and kernel = 'rbf', the arousal classifier accuracy improved from 0.591 to 0.635."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Include your code below \n",
    "Please make sure that your code is commented. If you performed analyses outside of this notebook (e.g., in a python script that needed more computational resources) please note that here and be sure to upload any accompanying notebook(s) or script(s) (submissions without accompanying code will be given zero points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pmc_nn.nii',\n",
       " 'intersect_mask.nii.gz',\n",
       " 'early_visual.nii',\n",
       " 'ddmn_mpfc.nii',\n",
       " 'a1_rev.nii',\n",
       " 'MNI152_T1_3mm_brain_mask.nii',\n",
       " 'aud_early.nii']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from brainiak import image, io\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "from brainiak import image, io\n",
    "from scipy.stats import stats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from finUtils import processregressor, remove_duplicate_rows, upsample_labels, mask_preprocess_data\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "os.listdir('/gpfs/gibbs/project/cmhn/data/Sherlock_processed/masks')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment Number</th>\n",
       "      <th>Scene Title</th>\n",
       "      <th>Start Time (s)</th>\n",
       "      <th>End Time (s)</th>\n",
       "      <th>Start TR</th>\n",
       "      <th>Scene Details - A Level</th>\n",
       "      <th>Scene Details - B Level</th>\n",
       "      <th>Original B Levels (Janice) fixed</th>\n",
       "      <th>Space-In/Outdoor</th>\n",
       "      <th>Who-N/G/M</th>\n",
       "      <th>...</th>\n",
       "      <th>Name - Focus</th>\n",
       "      <th>Name - Speaking</th>\n",
       "      <th>Location</th>\n",
       "      <th>Camera Angle</th>\n",
       "      <th>Words on Screen</th>\n",
       "      <th>Music Presence</th>\n",
       "      <th>Temporal Relationship</th>\n",
       "      <th>Type of Jump</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Cartoon Intro</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>People in popcorn, candy, and soft drink costu...</td>\n",
       "      <td>1. Cartoon introduction - singers repeat \"Let'...</td>\n",
       "      <td>1. Cartoon</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>Cartoon People in Costumes</td>\n",
       "      <td>Cartoon People in Costumes</td>\n",
       "      <td>Cartoon World</td>\n",
       "      <td>Long</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Popcorn is being popped in a large popcorn mac...</td>\n",
       "      <td>Segment 1-7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female Singer</td>\n",
       "      <td>Cartoon World</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Men sing in reply: \"the popcorn can't be beat!\"</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male Singers</td>\n",
       "      <td>Cartoon World</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>A family of four, a father with a black suit, ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Background Singers</td>\n",
       "      <td>Cartoon World</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>A view of the lobby with a display of snacks f...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>Cartoon Woman</td>\n",
       "      <td>Background Singers</td>\n",
       "      <td>Cartoon World</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>620.0</td>\n",
       "      <td>621.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>Mike: \"Sorry, it's in my coat.\" and walks to h...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>Mike</td>\n",
       "      <td>Mike</td>\n",
       "      <td>St. Bartholomew's Hospital Laboratory</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>621.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>Sherlock doesn't answer and peers down at his ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Sherlock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Bartholomew's Hospital Laboratory</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>622.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>John says: \"Umm here\" Sherlock turns his head ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Sherlock, John</td>\n",
       "      <td>John</td>\n",
       "      <td>St. Bartholomew's Hospital Laboratory</td>\n",
       "      <td>Over the Shoulder, Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>625.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>Sherlock says: \"Oh, thank you.\" (unexpectedly ...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Sherlock</td>\n",
       "      <td>Sherlock</td>\n",
       "      <td>St. Bartholomew's Hospital Laboratory</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>628.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>Mike says pointing at John: \"He's an old frien...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indoor</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>Mike</td>\n",
       "      <td>Mike</td>\n",
       "      <td>St. Bartholomew's Hospital Laboratory</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Segment Number    Scene Title   Start Time (s)    End Time (s)   \\\n",
       "0               1.0  Cartoon Intro               0.0            12.0   \n",
       "1               2.0             NaN             12.0            15.0   \n",
       "2               3.0             NaN             15.0            17.0   \n",
       "3               4.0             NaN             17.0            23.0   \n",
       "4               5.0             NaN             23.0            29.0   \n",
       "..              ...             ...              ...             ...   \n",
       "195           196.0             NaN            620.0           621.0   \n",
       "196           197.0             NaN            621.0           622.0   \n",
       "197           198.0             NaN            622.0           625.0   \n",
       "198           199.0             NaN            625.0           628.0   \n",
       "199           200.0             NaN            628.0           630.0   \n",
       "\n",
       "     Start TR                           Scene Details - A Level   \\\n",
       "0         0.0  People in popcorn, candy, and soft drink costu...   \n",
       "1         8.0  Popcorn is being popped in a large popcorn mac...   \n",
       "2        10.0    Men sing in reply: \"the popcorn can't be beat!\"   \n",
       "3        11.0  A family of four, a father with a black suit, ...   \n",
       "4        15.0  A view of the lobby with a display of snacks f...   \n",
       "..        ...                                                ...   \n",
       "195     413.0  Mike: \"Sorry, it's in my coat.\" and walks to h...   \n",
       "196     414.0  Sherlock doesn't answer and peers down at his ...   \n",
       "197     415.0  John says: \"Umm here\" Sherlock turns his head ...   \n",
       "198     417.0  Sherlock says: \"Oh, thank you.\" (unexpectedly ...   \n",
       "199     419.0  Mike says pointing at John: \"He's an old frien...   \n",
       "\n",
       "                               Scene Details - B Level  \\\n",
       "0    1. Cartoon introduction - singers repeat \"Let'...   \n",
       "1                                          Segment 1-7   \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "195                                                      \n",
       "196                                                      \n",
       "197                                                      \n",
       "198                                                      \n",
       "199                                                      \n",
       "\n",
       "    Original B Levels (Janice) fixed Space-In/Outdoor Who-N/G/M  ...  \\\n",
       "0                         1. Cartoon           Indoor         G  ...   \n",
       "1                                NaN           Indoor         G  ...   \n",
       "2                                NaN           Indoor         G  ...   \n",
       "3                                NaN           Indoor         G  ...   \n",
       "4                                NaN           Indoor         G  ...   \n",
       "..                               ...              ...       ...  ...   \n",
       "195                              NaN           Indoor         G  ...   \n",
       "196                              NaN           Indoor         M  ...   \n",
       "197                              NaN           Indoor         M  ...   \n",
       "198                              NaN           Indoor         M  ...   \n",
       "199                              NaN           Indoor         G  ...   \n",
       "\n",
       "                    Name - Focus              Name - Speaking  \\\n",
       "0    Cartoon People in Costumes   Cartoon People in Costumes    \n",
       "1                            NaN                Female Singer   \n",
       "2                            NaN                 Male Singers   \n",
       "3                            NaN           Background Singers   \n",
       "4                  Cartoon Woman           Background Singers   \n",
       "..                           ...                          ...   \n",
       "195                         Mike                         Mike   \n",
       "196                     Sherlock                          NaN   \n",
       "197               Sherlock, John                         John   \n",
       "198                     Sherlock                     Sherlock   \n",
       "199                         Mike                         Mike   \n",
       "\n",
       "                                  Location               Camera Angle  \\\n",
       "0                            Cartoon World                       Long   \n",
       "1                            Cartoon World                     Medium   \n",
       "2                            Cartoon World                     Medium   \n",
       "3                            Cartoon World                     Medium   \n",
       "4                            Cartoon World                     Medium   \n",
       "..                                     ...                        ...   \n",
       "195  St. Bartholomew's Hospital Laboratory                     Medium   \n",
       "196  St. Bartholomew's Hospital Laboratory                     Medium   \n",
       "197  St. Bartholomew's Hospital Laboratory  Over the Shoulder, Medium   \n",
       "198  St. Bartholomew's Hospital Laboratory                     Medium   \n",
       "199  St. Bartholomew's Hospital Laboratory                     Medium   \n",
       "\n",
       "    Words on Screen  Music Presence  Temporal Relationship  Type of Jump  \\\n",
       "0                NaN             Yes                    NaN          NaN   \n",
       "1                NaN             Yes                    NaN          NaN   \n",
       "2                NaN             Yes                    NaN          NaN   \n",
       "3                NaN             Yes                    NaN          NaN   \n",
       "4                NaN             Yes                    NaN          NaN   \n",
       "..               ...             ...                    ...          ...   \n",
       "195              NaN              No                    NaN          NaN   \n",
       "196              NaN              No                    NaN          NaN   \n",
       "197              NaN              No                    NaN          NaN   \n",
       "198              NaN              No                    NaN          NaN   \n",
       "199              NaN              No                    NaN          NaN   \n",
       "\n",
       "     Arousal  valence  \n",
       "0        3.0        +  \n",
       "1        3.0        +  \n",
       "2        3.0        +  \n",
       "3        3.0        +  \n",
       "4        3.0        +  \n",
       "..       ...      ...  \n",
       "195      3.0        +  \n",
       "196      3.0        +  \n",
       "197      3.0        +  \n",
       "198      3.0        +  \n",
       "199      3.0        +  \n",
       "\n",
       "[200 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#establish data directory\n",
    "data_dir ='/gpfs/gibbs/project/cmhn/data/Sherlock_processed/'\n",
    "\n",
    "# declare and display the regressors data\n",
    "df = pd.read_csv(data_dir + 'regressor_file.csv') # pandas plays best with csv files\n",
    "df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare constants for part 1\n",
    "num_subs = 17\n",
    "subs = np.arange(num_subs)\n",
    "numTRs = 946\n",
    "\n",
    "#load in masks\n",
    "\n",
    "# a1_rev (first auditory area)\n",
    "a1_mask_file= data_dir + 'masks/a1_rev.nii'\n",
    "a1_mask = io.load_boolean_mask(a1_mask_file)\n",
    "\n",
    "# aud_early (early auditory cortex)\n",
    "aud_mask_file= data_dir + 'masks/aud_early.nii'\n",
    "aud_mask = io.load_boolean_mask(aud_mask_file)\n",
    "\n",
    "# whole brain mask\n",
    "mask_file= data_dir + 'masks/MNI152_T1_3mm_brain_mask.nii'\n",
    "brain_mask = io.load_boolean_mask(mask_file)\n",
    "\n",
    "# intersect mask\n",
    "intersect_mask_file= data_dir + 'masks/intersect_mask.nii.gz'\n",
    "intersect_mask = io.load_boolean_mask(intersect_mask_file)\n",
    "\n",
    "# early_visual (early visual cortex)\n",
    "visual_mask_file= data_dir + 'masks/early_visual.nii'\n",
    "visual_mask = io.load_boolean_mask(visual_mask_file)\n",
    "\n",
    "# ddmn_mpfc (medial prefontal cortex)\n",
    "mpfc_mask_file= data_dir + 'masks/ddmn_mpfc.nii'\n",
    "mpfc_mask = io.load_boolean_mask(mpfc_mask_file)\n",
    "\n",
    "# pmc mask (posterior medial cortex)\n",
    "pmc_mask_file= data_dir + 'masks/pmc_nn.nii'\n",
    "pmc_mask = io.load_boolean_mask(pmc_mask_file)\n",
    "\n",
    "# this will be useful for later masking of data\n",
    "mask_names = ['a1_mask', 'aud_mask', 'brain_mask', 'intersect_mask', 'mpfc_mask', 'pmc_mask', 'visual_mask']\n",
    "masks = [a1_mask, aud_mask, brain_mask, intersect_mask, mpfc_mask, pmc_mask, visual_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in labels\n",
    "start = df['Start TR']\n",
    "location = df['Location']\n",
    "regressors = list(df.columns)\n",
    "startTR_array = start.values\n",
    "\n",
    "regressors = ['Space-In/Outdoor', 'valence', 'arousal']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create arrays of label data for each of the regressors after binarizing the entries in the regressors csv to 1 and 0. We need to make sure that there is a sufficiently large number of TRs with 1 and a large number of TRs with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of + TRs = 1143\n",
      "Number of - TRs = 276\n",
      "Total TRs = 1419\n"
     ]
    }
   ],
   "source": [
    "# load in valence labels\n",
    "valence = processregressor('valence', startTR_array, df)\n",
    "\n",
    "i = 0 # initalize counter\n",
    "while i < valence.shape[0]: \n",
    "    \n",
    "    # + is 1, - is 0\n",
    "    if valence[i][1] == '+':\n",
    "        valence[i][1] = 1\n",
    "    elif valence[i][1] == '-':\n",
    "        valence[i][1] = 0\n",
    "    \n",
    "    # if not yes or no, remove the row entirely\n",
    "    else: \n",
    "        valence = np.delete(valence, i, axis=0)\n",
    "        i -= 1\n",
    "    i += 1\n",
    "valence = remove_duplicate_rows(valence) # remove duplicate rows\n",
    "valence = upsample_labels(valence, numTRs) # upsample_labels\n",
    "      \n",
    "print('Number of + TRs =', (valence[:,1] == 1).sum())\n",
    "print('Number of - TRs =', (valence[:,1] == 0).sum()) \n",
    "print('Total TRs =', valence.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Indoor TRs = 992\n",
      "Number of Outdoor TRs = 427\n",
      "Total TRs = 1419\n"
     ]
    }
   ],
   "source": [
    "# in/outdoor\n",
    "start = df['Start TR']\n",
    "\n",
    "inout = processregressor('Space-In/Outdoor', startTR_array, df)\n",
    "\n",
    "# make labels boolean and remove any extraneous rows \n",
    "i = 0 # initalize counter\n",
    "while i < inout.shape[0]: \n",
    "    \n",
    "    \n",
    "    # indoor is 1, outdoor is 0\n",
    "    if inout[i][1] == 'Indoor':\n",
    "        inout[i][1] = 1\n",
    "    elif inout[i][1] == 'Outdoor':\n",
    "        inout[i][1] = 0\n",
    "    \n",
    "    # if not indoor or outdoor, remove the row entirely\n",
    "    else: \n",
    "        inout = np.delete(inout, i, axis=0)\n",
    "        i -= 1\n",
    "\n",
    "    i += 1\n",
    "    # print(indoor_outdoor)\n",
    "\n",
    "inout = remove_duplicate_rows(inout)\n",
    "inout = upsample_labels(inout, numTRs)\n",
    "    \n",
    "# print(np.shape(indoor_outdoor), indoor_outdoor[..., 1])\n",
    "print('Number of Indoor TRs =',(inout[:,1] == 1).sum())\n",
    "print('Number of Outdoor TRs =', (inout[:,1] == 0).sum())\n",
    "print('Total TRs =', inout.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of High Arousal TRs = 873\n",
      "Number of Low Arousal TRs = 546\n",
      "Total TRs = 1419\n"
     ]
    }
   ],
   "source": [
    "# arousal\n",
    "arousal = processregressor('Arousal', startTR_array, df)\n",
    "\n",
    "i = 0 # initalize counter\n",
    "while i < arousal.shape[0]: \n",
    "    \n",
    "    # + is 1, - is 0\n",
    "    if arousal[i][1] >= 4:\n",
    "        arousal[i][1] = 1\n",
    "    elif arousal[i][1] <= 3:\n",
    "        arousal[i][1] = 0\n",
    "    \n",
    "    # if not yes or no, remove the row entirely\n",
    "    else: \n",
    "        arousal = np.delete(arousal, i, axis=0)\n",
    "        i -= 1\n",
    "    i += 1\n",
    "    \n",
    "arousal = remove_duplicate_rows(arousal) # remove duplicate rows\n",
    "arousal = upsample_labels(arousal, numTRs) # upsample_labels\n",
    "      \n",
    "print('Number of High Arousal TRs =', (arousal[:,1] == 1).sum())\n",
    "print('Number of Low Arousal TRs =', (arousal[:,1] == 0).sum())\n",
    "print('Total TRs =', arousal.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in, normalize, mask, and save the data to our palmer scratch directory for easy and quick access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_dir = data_dir + 'part_1/'\n",
    "# p1data = [] # add 1 to index to get sub no.\n",
    "# for i in range(1, 18):\n",
    "#     p1data.append(nib.load(p1_dir + f'sub-{i:02d}_sherlock_movie_part1.nii'))\n",
    "\n",
    "# for i in range(len(mask_names)):\n",
    "#     mask_preprocess_data(p1data, mask_names[i], masks[i], num_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1_mask_data.npy',\n",
       " 'aud_mask_data.npy',\n",
       " 'brain_mask_data.npy',\n",
       " 'intersect_mask_data.npy',\n",
       " 'mpfc_mask_data.npy',\n",
       " 'pmc_mask_data.npy',\n",
       " 'visual_mask_data.npy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/home/cmhn_ak2776/palmer_scratch/final_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create arrays in this notebook that contain the bold activation for all 17 subjects stored in a 2D array with one dimension being TRs and one dimension being voxel.\n",
    "\n",
    "def load_maskedData(file_path):\n",
    "    data = np.load(file_path)\n",
    "    return np.swapaxes(data, 1, 2)\n",
    "\n",
    "maskedDataDir = \"/home/cmhn_ak2776/palmer_scratch/final_project/\"\n",
    "    \n",
    "a1_maskedData_normalized = load_maskedData(maskedDataDir + \"a1_mask_data.npy\")\n",
    "aud_maskedData_normalized = load_maskedData(maskedDataDir + \"aud_mask_data.npy\")\n",
    "brain_maskedData_normalized = load_maskedData(maskedDataDir + \"brain_mask_data.npy\")\n",
    "intersect_maskedData_normalized = load_maskedData(maskedDataDir + \"intersect_mask_data.npy\")\n",
    "mpfc_maskedData_normalized = load_maskedData(maskedDataDir + \"mpfc_mask_data.npy\")\n",
    "pmc_maskedData_normalized = load_maskedData(maskedDataDir + \"pmc_mask_data.npy\")\n",
    "visual_maskedData_normalized = load_maskedData(maskedDataDir + \"visual_mask_data.npy\")\n",
    "\n",
    "maskedDataNamesList = ['a1_maskedData_normalized', 'aud_maskedData_normalized', 'brain_maskedData_normalized', 'intersect_maskedData_normalized', 'mpfc_maskedData_normalized', 'pmc_maskedData_normalized', 'visual_maskedData_normalized']\n",
    "maskedDataList = [a1_maskedData_normalized, aud_maskedData_normalized, brain_maskedData_normalized, intersect_maskedData_normalized, mpfc_maskedData_normalized, pmc_maskedData_normalized, visual_maskedData_normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 946, 174)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here you can see the shape of one such variable\n",
    "a1_maskedData_normalized.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test to see the best mask for every regressor in order to select the mask for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegressorList = [inout, valence, arousal]\n",
    "RegressorNameList = ['inout', 'valence', 'arousal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Regressor inout and mask a1_maskedData_normalized mean accuracy =  0.6046511627906976\n",
      "for Regressor inout and mask aud_maskedData_normalized mean accuracy =  0.5285412262156448\n",
      "for Regressor inout and mask brain_maskedData_normalized mean accuracy =  0.547568710359408\n",
      "for Regressor inout and mask intersect_maskedData_normalized mean accuracy =  0.5528541226215645\n",
      "for Regressor inout and mask mpfc_maskedData_normalized mean accuracy =  0.5200845665961945\n",
      "for Regressor inout and mask pmc_maskedData_normalized mean accuracy =  0.5813953488372093\n",
      "for Regressor inout and mask visual_maskedData_normalized mean accuracy =  0.63107822410148\n",
      "for Regressor valence and mask a1_maskedData_normalized mean accuracy =  0.864693446088795\n",
      "for Regressor valence and mask aud_maskedData_normalized mean accuracy =  0.7241014799154334\n",
      "for Regressor valence and mask brain_maskedData_normalized mean accuracy =  0.813953488372093\n",
      "for Regressor valence and mask intersect_maskedData_normalized mean accuracy =  0.813953488372093\n",
      "for Regressor valence and mask mpfc_maskedData_normalized mean accuracy =  0.7124735729386892\n",
      "for Regressor valence and mask pmc_maskedData_normalized mean accuracy =  0.864693446088795\n",
      "for Regressor valence and mask visual_maskedData_normalized mean accuracy =  0.864693446088795\n",
      "for Regressor arousal and mask a1_maskedData_normalized mean accuracy =  0.5613107822410148\n",
      "for Regressor arousal and mask aud_maskedData_normalized mean accuracy =  0.5919661733615222\n",
      "for Regressor arousal and mask brain_maskedData_normalized mean accuracy =  0.6099365750528541\n",
      "for Regressor arousal and mask intersect_maskedData_normalized mean accuracy =  0.5919661733615222\n",
      "for Regressor arousal and mask mpfc_maskedData_normalized mean accuracy =  0.5158562367864693\n",
      "for Regressor arousal and mask pmc_maskedData_normalized mean accuracy =  0.554968287526427\n",
      "for Regressor arousal and mask visual_maskedData_normalized mean accuracy =  0.5591966173361522\n"
     ]
    }
   ],
   "source": [
    "# iterate through all masks and all regressors to determine the best mask for each regressor we are trying to classify\n",
    "for r in range(len(RegressorList)):\n",
    "    for m in range(len(maskedDataList)):\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        train_data = np.mean(maskedDataList[m][range(16),...], axis = 0) # average over all but the held-out subject, sub 16\n",
    "        test_data = maskedDataList[m][16,...] # test on the 17th subject\n",
    "        labels = RegressorList[r][:946, 1].astype('int')\n",
    "\n",
    "        classifier = SVC(C = 0.01, gamma = 0.01, kernel = 'linear')\n",
    "        clf = classifier.fit(train_data, labels)\n",
    "\n",
    "        score = clf.score(test_data, labels)\n",
    "\n",
    "        print('for Regressor', RegressorNameList[r], 'and mask', maskedDataNamesList[m], 'mean accuracy = ', score) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, we can see that:  \n",
    "  \n",
    "the best mask for predicting Indoor/Outdoor Scenes is the visual mask with an accuracy of 0.63, with chance being 0.5  \n",
    "  \n",
    "the best mask for predicting Valence is tied between a1_maskedData, pmc_maskedData, and visual_maskedData all at accuracy = 0.865  \n",
    "  \n",
    "the best mask for predicting Arousal is the whole brain mask with an accuracy of 0.61, with chance being 0.5, however since the accuracy is low and this is a large mask that takes very long to run in a classifier, we can utilize the much smaller early auditory cortex (aud) mask for further optimization, which had an accuracy of 0.591. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.63107822410148\n"
     ]
    }
   ],
   "source": [
    "# classifier 1: inout\n",
    "np.random.seed(0)\n",
    "train_data = np.mean(visual_maskedData_normalized[range(16),...], axis = 0) # average over all but held-out participant\n",
    "test_data = visual_maskedData_normalized[16,...] # test on the 17th subject\n",
    "labels = inout[:946, 1].astype('int')\n",
    "\n",
    "classifier = SVC(C = 0.01, gamma = 0.01, kernel = 'linear')\n",
    "clf = classifier.fit(train_data, labels)\n",
    "\n",
    "inout_score = clf.score(test_data, labels)\n",
    "    \n",
    "\n",
    "print('accuracy: ', inout_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.864693446088795\n"
     ]
    }
   ],
   "source": [
    "# classifier 2: valence\n",
    "np.random.seed(0)\n",
    "train_data = np.mean(pmc_maskedData_normalized[range(16),...], axis = 0) # average over all but held-out participant\n",
    "test_data = pmc_maskedData_normalized[16,...] # test on the 17th subject\n",
    "labels = valence[:946, 1].astype('int')\n",
    "\n",
    "classifier = SVC(C = 0.01, gamma = 0.01, kernel = 'linear')\n",
    "clf = classifier.fit(train_data, labels)\n",
    "\n",
    "valence_score = clf.score(test_data, labels)\n",
    "    \n",
    "\n",
    "print('accuracy: ', valence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5919661733615222\n"
     ]
    }
   ],
   "source": [
    "# classifier 3: arousal\n",
    "np.random.seed(0)\n",
    "train_data = np.mean(aud_maskedData_normalized[range(16),...], axis = 0) # average over all but held-out participant\n",
    "test_data = aud_maskedData_normalized[16,...]\n",
    "labels = arousal[:946, 1].astype('int')\n",
    "\n",
    "classifier = SVC(C = 0.01, gamma = 0.01, kernel = 'linear')\n",
    "clf = classifier.fit(train_data, labels)\n",
    "\n",
    "arousal_score = clf.score(test_data, labels)\n",
    "    \n",
    "\n",
    "print('accuracy: ', np.mean(arousal_score)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is too low. Let's try to improve it using a grid search for best classifier params and a select k best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5655391120507399 k = 40 params = {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "accuracy:  0.620507399577167 k = 80 params = {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "accuracy:  0.5158562367864693 k = 120 params = {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "accuracy:  0.5126849894291755 k = 160 params = {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "accuracy:  0.6247357293868921 k = 200 params = {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "accuracy:  0.6236786469344608 k = 240 params = {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "accuracy:  0.635306553911205 k = 280 params = {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "accuracy:  0.6004228329809725 k = 320 params = {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "accuracy:  0.5771670190274841 k = 360 params = {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# arousal optimization\n",
    "np.random.seed(0)\n",
    "train_data = np.mean(aud_maskedData_normalized[range(16),...], axis = 0) # average over all but held-out participant\n",
    "test_data = aud_maskedData_normalized[16,...]\n",
    "labels = arousal[:946, 1].astype('int')\n",
    "\n",
    "for k in range(40, 400, 40):\n",
    "    # Feature selection\n",
    "    k = k\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    train_data_selected = selector.fit_transform(train_data, labels)\n",
    "    test_data_selected = selector.transform(test_data)\n",
    "\n",
    "    # Hyperparameter tuning with GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1],\n",
    "        'gamma': [0.001, 0.01, 0.1],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "    grid_search.fit(train_data_selected, labels)\n",
    "\n",
    "    # Train the classifier with the best parameters\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "    best_classifier.fit(train_data_selected, labels)\n",
    "\n",
    "    # Test the classifier\n",
    "    arousal_score = best_classifier.score(test_data_selected, labels)\n",
    "\n",
    "    print('accuracy: ', np.mean(arousal_score), 'k =', k, 'params =', grid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, utilizing GridSearchCV and SelectKBest, we were able to determine that selecting for the k = 280 best features and with the classifier parameters set to C = 0.1, gamma = 0.01, and kernel = 'rbf', the arousal classifier accuracy improved from 0.591 to 0.635."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an exploratory analysis of whether arousal can predict the location in the first part of the data as the outdoor scenes here contain a high arousal war scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3. 4. 4. 5. 5. 5.]\n",
      "['Indoor' 'Indoor' 'Indoor' 'Indoor' 'Indoor' 'Indoor' 'Indoor' 'Outdoor'\n",
      " 'Outdoor' 'Outdoor']\n",
      "Accuracy = 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Exploratory NOT FOR GRADING: uses arousal to predict indoor/outdoor\n",
    "\n",
    "arousaltest = df['Arousal']\n",
    "\n",
    "# To turn the column into a vector of values:\n",
    "print(arousaltest.values[:10])\n",
    "\n",
    "loc = df['Space-In/Outdoor']\n",
    "\n",
    "# To turn the column into a vector of values:\n",
    "print(loc.values[:10]) # Just show the first 10 values.\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_test = arousaltest.values[:200].reshape(-1,1)\n",
    "y_test = loc.values[:200].reshape(-1,1)\n",
    "X_train = arousaltest.values[200:400].reshape(-1,1)\n",
    "y_train = loc.values[200:400].reshape(-1,1)\n",
    "\n",
    "# Create a linear SVC model with hyperparameter C set to 1.  \n",
    "model = LinearSVC(C=1)\n",
    "\n",
    "# Fit the model on the BOLD data and corresponding stimulus labels.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Score your model on the test data.\n",
    "score = model.score(X_test, y_test)\n",
    "print(f'Accuracy = {score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mystery Segment Matching\n",
    "\n",
    "For participants 2 through 17, you are given the first 300 TRs (450s) of continuous data from viewing part 2 of the movie. For participant 1, you have just a mystery segment: a 30-TR (45s) chunk pulled from those same 300 TRs of the movie. Your goal is to figure out _when_ in those first 300 TRs your mystery segment begins, based on the data from all of the _other_ participants. Your answer will be the starting TR of the 30-TR mystery segment, which is some timepoint between 1 and 270. \n",
    "\n",
    "**Data directory:** Data can be found in `/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/`. Remember, participant 1 is a volume of 30 TRs, and all other participants are volumes of 300 TRs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the starting TR of the mystery epoch.\n",
    "Justify your response with supporting evidence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  121 (but maybe 128) (note there is another copy of this next to the last code chunk that makes it easier to see the reasoning)\n",
    "  \n",
    "Looking at the results from the last code chunk, it seems the start TR is somewhere between 121 and 128. The huge difference in measures of center for both auditory masks might be due to some sort of noise that is unaccounted for such as a particularly loud noise from the MRI machine at a certain TR that causes all subs to synchronize in auditory activity and thus makes them both undesirable for this analysis.  \n",
    "  \n",
    "The visual mask had the same median and mode, which is promising and the high mean might be due to outliers that caused synchronised visual stimulation at a time that is irrelevant to our analysis from something other than the sherlock data. The pmc and mpfc masks give somewhat promising data and the mean for these is unlikely to be induced by outliers as simple as loud noises or visual stimulation and thus capturing attention and emotion and memories in synchrony is likely not influenced by outlier occurances at random times.  \n",
    "  \n",
    "In order to give a single answer, I will state that my predicted TR is 121 as it was both the median and mode of the visual mask and falls nicely between the TR predictions. (it might still be 128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include your code below\n",
    "Please make sure that your code is commented. If you performed analyses outside of this notebook (e.g., in a python script that needed more computational resources) please note that here and be sure to upload any accompanying notebook(s) or script(s) (submissions without accompanying code will be given zero points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.signal import correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-12_segment_q2.nii', 'sub-17_segment_q2.nii', 'sub-06_segment_q2.nii', 'sub-11_segment_q2.nii', 'sub-03_segment_q2.nii', 'sub-09_segment_q2.nii', 'sub-04_segment_q2.nii', 'sub-16_segment_q2.nii', 'sub-05_segment_q2.nii', 'sub-14_segment_q2.nii', 'sub-07_segment_q2.nii', 'sub-10_segment_q2.nii', 'sub-08_segment_q2.nii', 'sub-13_segment_q2.nii', 'sub-02_segment_q2.nii', 'sub-01_segment_q2.nii', 'sub-15_segment_q2.nii']\n"
     ]
    }
   ],
   "source": [
    "pt2_dir = data_dir + 'mystery_segments_q2/'\n",
    "print(os.listdir(pt2_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2files = [pt2_dir + 'sub-12_segment_q2.nii', pt2_dir + 'sub-17_segment_q2.nii', pt2_dir + 'sub-06_segment_q2.nii', pt2_dir + 'sub-11_segment_q2.nii', pt2_dir + 'sub-03_segment_q2.nii', pt2_dir + 'sub-09_segment_q2.nii', pt2_dir+'sub-04_segment_q2.nii', pt2_dir+'sub-16_segment_q2.nii', pt2_dir+'sub-05_segment_q2.nii', pt2_dir+'sub-14_segment_q2.nii', pt2_dir+'sub-07_segment_q2.nii', pt2_dir+'sub-10_segment_q2.nii', pt2_dir+'sub-08_segment_q2.nii', pt2_dir+'sub-13_segment_q2.nii', pt2_dir+'sub-02_segment_q2.nii', pt2_dir+'sub-01_segment_q2.nii', pt2_dir+'sub-15_segment_q2.nii']\n",
    "subjects_data = [nib.load(file).get_fdata() for file in p2files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-12_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-17_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-06_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-11_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-03_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-09_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-04_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-16_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-05_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-14_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-07_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-10_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-08_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-13_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-02_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-01_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-15_segment_q2.nii']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_01_data = subjects_data[15]\n",
    "rest_subjects_data = np.stack(subjects_data[:15] + [subjects_data[16]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 73, 61, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_01_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 61, 73, 61, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_subjects_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I could not find out how to properly apply the time segment matching function from pset 11 or properly figure out using correlations to determine similarities and thus found an approach using mean squared error with a slightly different application strategy. This will likely have flaws but seemed logical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(segment_1, segment_2):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error between two time series (segments).\n",
    "\n",
    "    Params:\n",
    "    segment_1 (numpy.ndarray): The first time series (1D array).\n",
    "    segment_2 (numpy.ndarray): The second time series (1D array) of the same length as the first one.\n",
    "\n",
    "    Returns:\n",
    "    float: The mean squared error between the two time series.\n",
    "    \"\"\"\n",
    "    return np.mean((segment_1 - segment_2) ** 2)\n",
    "\n",
    "def time_segment_matching(sub_01_ts, subject_ts):\n",
    "    \"\"\"\n",
    "    Find the best matching time segment in a longer time series for a given shorter time series.\n",
    "\n",
    "    Params:\n",
    "    sub_01_ts (numpy.ndarray): The shorter time series (1D array).\n",
    "    subject_ts (numpy.ndarray): The longer time series (1D array) to search for the best matching segment.\n",
    "\n",
    "    Returns:\n",
    "    int: The starting index of the best matching time segment in the longer time series.\n",
    "    \"\"\"\n",
    "    num = len(subject_ts) - len(sub_01_ts) + 1\n",
    "    mse_values = np.zeros(num)\n",
    "\n",
    "    for TR in range(num):\n",
    "        mse_values[TR] = mse(sub_01_ts, subject_ts[TR:TR + len(sub_01_ts)])\n",
    "\n",
    "    best_TR = np.argmin(mse_values)\n",
    "    return best_TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely Starting TR of the 30 TRs in the 300 TRs is at TR: 49 according to a1_mask\n",
      "The average of best_timepoints_masked was 69.91379310344827\n",
      "The median was 49.0\n",
      "The most likely Starting TR of the 30 TRs in the 300 TRs is at TR: 145 according to aud_mask\n",
      "The average of best_timepoints_masked was 91.23673870333988\n",
      "The median was 78.0\n",
      "The most likely Starting TR of the 30 TRs in the 300 TRs is at TR: 121 according to visual_mask\n",
      "The average of best_timepoints_masked was 144.8827361563518\n",
      "The median was 121.0\n",
      "The most likely Starting TR of the 30 TRs in the 300 TRs is at TR: 112 according to pmc_mask\n",
      "The average of best_timepoints_masked was 128.9085239085239\n",
      "The median was 130.0\n",
      "The most likely Starting TR of the 30 TRs in the 300 TRs is at TR: 128 according to mpfc_mask\n",
      "The average of best_timepoints_masked was 122.26028169014084\n",
      "The median was 129.0\n"
     ]
    }
   ],
   "source": [
    "MaskList = [a1_mask, aud_mask, visual_mask, pmc_mask, mpfc_mask]\n",
    "MaskNames = ['a1_mask', 'aud_mask', 'visual_mask','pmc_mask', 'mpfc_mask']\n",
    "for m in range(len(MaskList)):\n",
    "    # Get the dimensions of the sub-01 data (x_dim, y_dim, and z_dim for the brain voxels)\n",
    "    x_dim, y_dim, z_dim, _ = sub_01_data.shape\n",
    "\n",
    "    # Set the length of the time segment to compare\n",
    "    segment_length = 30\n",
    "\n",
    "    # Get the indices of the brain voxels within the masks\n",
    "    brain_voxels = np.argwhere(MaskList[m])\n",
    "\n",
    "    # Initialize the best_timepoints array with -1 to indicate unused voxels. This array will store the best matching time point (BTR) for each voxel within the mask\n",
    "    best_timepoints = np.full((x_dim, y_dim, z_dim), -1, dtype=int)\n",
    "\n",
    "    other_subjects_data = np.mean(rest_subjects_data, axis = 0)\n",
    "    \n",
    "    # Loop through each voxel within the visual_mask\n",
    "    for x, y, z in brain_voxels:\n",
    "        # Initialize the minimum mean squared error (MSE) to a large value\n",
    "        min_mse = float('inf')\n",
    "        best_BTR = 0\n",
    "\n",
    "        # Extract the time series for the current voxel in sub-01 for the specified segment_length\n",
    "        sub_01_ts = sub_01_data[x, y, z, :]\n",
    "        \n",
    "        subjects_ts = other_subjects_data[x, y, z, :]\n",
    "\n",
    "        # Find the best matching time point (BTR) for the current subject using the time_segment_matching function\n",
    "        BTR = time_segment_matching(sub_01_ts, subjects_ts)\n",
    "\n",
    "        # Calculate the mean squared error for the best matching time segment in the current subject\n",
    "        current_mse = mse(sub_01_ts, subjects_ts[BTR:BTR + segment_length])\n",
    "\n",
    "        # If the current mean squared error is smaller than the minimum mean squared error found so far, update the minimum mean squared error and the best BTR\n",
    "        if current_mse < min_mse:\n",
    "            min_mse = current_mse\n",
    "            best_BTR = BTR\n",
    "\n",
    "        # Store the best matching time point (BTR) for the current voxel in the best_timepoints array\n",
    "        best_timepoints[x, y, z] = best_BTR\n",
    "\n",
    "    from scipy.stats import mode\n",
    "\n",
    "    # Get the best timepoints for the voxels within the aud_mask\n",
    "    best_timepoints_masked = best_timepoints[MaskList[m]]\n",
    "    \n",
    "    # Calculate the mode of the best_timepoints_masked\n",
    "    location_mode, count = mode(best_timepoints_masked)\n",
    "\n",
    "    print(f\"The most likely Starting TR of the 30 TRs in the 300 TRs is at TR: {location_mode[0]} according to {MaskNames[m]}\")\n",
    "    print(f\"The average of best_timepoints_masked was {np.mean(best_timepoints_masked)}\")\n",
    "    print(f\"The median was {np.median(best_timepoints_masked)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  121 (but maybe 128) (note there is another copy of this next to the last code chunk that makes it easier to see the reasoning)\n",
    "  \n",
    "Looking at the results from the last code chunk, it seems the start TR is somewhere between 121 and 128. The huge difference in measures of center for both auditory masks might be due to some sort of noise that is unaccounted for such as a particularly loud noise from the MRI machine and thus makes them both undesirable for this analysis.  \n",
    "  \n",
    "The visual mask had the same median and mode, which is promising and the high mean might be due to outliers that caused synchronised visual stimulation at a time that is irrelevant to our analysis from something other than the sherlock data. The pmc and mpfc masks give somewhat promising data and the mean for these is unlikely to be induced by outliers as simple as loud noises or visual stimulation and thus capturing attention and emotion and memories in synchrony is likely not influenced by outlier occurances at random times.  \n",
    "  \n",
    "In order to give a single answer, I will state that my predicted TR is 121 as it was both the median and mode of the visual mask and falls nicely between the TR predictions. (it might still be 128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mystery Segment Decoding\n",
    "\n",
    "You will be given a mystery segment of 100 TRs from all participants in Part 2 of the movie. All participants' volumes are for the same 100 TRs, and you must figure out when in the movie this epoch occurred. Note that this segment does not overlap with the 300 TRs used in Question 2 - that means the starting TR could be between TRs 301 and 930. To determine the location of the mystery segment, use the classifiers you developed in Question 1 to make predictions about regressors for the mystery segment. Then, identify the location of the mystery segment by comparing the predicted regressors to the actual regressors for Part 2 (provided in the regressors file) and seeing what position of the mystery segment yields the best match between predicted and actual regressors. You need to provide code that does this matching and also quantitative results that justify your choice of mystery segment location. \n",
    "\n",
    "Again, please do not directly share your guess about the mystery segment with other students (it’s fine to share your ideas about analysis approaches with other students, but don’t share the final answer).\n",
    "\n",
    "**Data directory:** Data can be found in `/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q3/`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the starting TR of the mystery epoch.\n",
    "Justify your response with supporting evidence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** 404; the start TR for which our SVC models' predictions of regressor values were most accurate for the following 100 TRs was selected to be 404."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include your code below\n",
    "Please make sure that your code is commented. If you performed analyses outside of this notebook (e.g., in a python script that needed more computational resources) please note that here and be sure to upload any accompanying notebook(s) or script(s) (submissions without accompanying code will be given zero points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<nibabel.nifti1.Nifti1Image at 0x2b88ffa57e90>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88ff829750>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88ff9cbf90>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88ff73e590>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88ffa8f1d0>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b8b330d9d50>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b8b330d9d10>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b8b330d9dd0>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b8b330df710>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b8b330df510>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b8b330df1d0>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b8b330df090>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88bfeb3310>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88bfeb3650>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88bfeb3c90>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88bfeb3710>,\n",
       " <nibabel.nifti1.Nifti1Image at 0x2b88ff8aa6d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects_data_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-06_segment_q3.nii', 'sub-04_segment_q3.nii', 'sub-09_segment_q3.nii', 'sub-07_segment_q3.nii', 'sub-14_segment_q3.nii', 'sub-13_segment_q3.nii', 'sub-10_segment_q3.nii', 'sub-12_segment_q3.nii', 'sub-11_segment_q3.nii', 'sub-02_segment_q3.nii', 'sub-05_segment_q3.nii', 'segment_q3_part2.nii', 'sub-17_segment_q3.nii', 'sub-03_segment_q3.nii', 'sub-08_segment_q3.nii', 'sub-01_segment_q3.nii', 'sub-16_segment_q3.nii', 'sub-15_segment_q3.nii']\n",
      "(100, 307)\n",
      "(100, 1018)\n",
      "(100, 481)\n"
     ]
    }
   ],
   "source": [
    "# Set the directory path for the mystery_segments_q3 folder\n",
    "mystery_segments_folder = data_dir + 'mystery_segments_q3/'\n",
    "\n",
    "# Print the list of files in the mystery_segments_folder\n",
    "print(os.listdir(mystery_segments_folder))\n",
    "\n",
    "# Initialize an empty list to store the subject data\n",
    "subjects_data_q3 = []\n",
    "\n",
    "# Load the NIfTI images for subjects 1 to 17 and append them to subjects_data_q3\n",
    "for i in range(1, 18):\n",
    "    nifti_image = nib.load(mystery_segments_folder + f'sub-{i:02d}_segment_q3.nii')\n",
    "    subjects_data_q3.append(nifti_image)\n",
    "\n",
    "# Define a function to mask and preprocess data for all subjects\n",
    "def mask_scale_q3(data, mask, num_subs):\n",
    "    \"\"\"Mask and preprocess data for all subjects.\n",
    "\n",
    "    Params:\n",
    "    data: 4D fMRI data\n",
    "    mask: boolean mask\n",
    "    num_subs: number of subjects\n",
    "    \n",
    "    Returns:\n",
    "    all_subject_data: array of all the bold data for the mask chosen for all subs\n",
    "    \"\"\"\n",
    "    all_subject_data = []\n",
    "    for subject in range(num_subs):\n",
    "        masked_data = image.mask_image(data[subject], mask)\n",
    "        scaler = preprocessing.StandardScaler().fit(masked_data)\n",
    "        preprocessed_data = scaler.transform(masked_data)\n",
    "        all_subject_data.append(np.swapaxes(preprocessed_data, 0, 1))\n",
    "    return all_subject_data\n",
    "        \n",
    "# Set the visual mask\n",
    "visual_mask = visual_mask\n",
    "\n",
    "# Extract and preprocess the visual data for all subjects\n",
    "extracted_visual_data = mask_scale_q3(subjects_data_q3, visual_mask, len(subjects_data_q3))\n",
    "\n",
    "# Compute the mean of the extracted_visual_data across subjects\n",
    "avg_visual_segment = np.mean(extracted_visual_data, axis = 0)\n",
    "\n",
    "# Print the shape of the average visual segment\n",
    "print(np.shape(avg_visual_segment))\n",
    "\n",
    "# Set the audio mask\n",
    "audio_mask = aud_mask\n",
    "\n",
    "# Extract and preprocess the audio data for all subjects\n",
    "extracted_audio_data = mask_scale_q3(subjects_data_q3, audio_mask, len(subjects_data_q3))\n",
    "\n",
    "# Compute the mean of the extracted_audio_data across subjects\n",
    "avg_audio_segment = np.mean(extracted_audio_data, axis = 0)\n",
    "\n",
    "# Print the shape of the average audio segment\n",
    "print(np.shape(avg_audio_segment))\n",
    "\n",
    "# Set the PMC mask\n",
    "pmc_mask = pmc_mask\n",
    "\n",
    "# Extract and preprocess the PMC data for all subjects\n",
    "extracted_pmc_data = mask_scale_q3(subjects_data_q3, pmc_mask, len(subjects_data_q3))\n",
    "\n",
    "# Compute the mean of the extracted_pmc_data across subjects\n",
    "avg_pmc_segment = np.mean(extracted_pmc_data, axis = 0)\n",
    "\n",
    "# Print the shape of the average PMC segment\n",
    "print(np.shape(avg_pmc_segment))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create SVCs to create predictions for the indoor/outdoor value, valence, and arousal for each TR in the mystery 100 TR segment so that we can determine where in the 301 - 930 TRs the 100 TRs fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Use a SVC to predict inout from the average visual segment\n",
    "visual_data = np.mean(visual_maskedData_normalized, axis=0)\n",
    "inout_labels = inout[:946, 1].astype('int')\n",
    "linear_svm = SVC(C=0.01, gamma=0.01, kernel='linear')\n",
    "clf = linear_svm.fit(visual_data, inout_labels)\n",
    "inout_pred = clf.predict(avg_visual_segment)\n",
    "\n",
    "# Use a SVC to predict valence from the average PMC segment\n",
    "pmc_data = np.mean(pmc_maskedData_normalized, axis=0)\n",
    "valence_labels = valence[:946, 1].astype('int')\n",
    "clf = SVC(C=0.01, gamma=0.01, kernel='linear')\n",
    "classifier = clf.fit(pmc_data, valence_labels)\n",
    "valence_pred = classifier.predict(avg_pmc_segment)\n",
    "\n",
    "# Use a SVC to predict arousal from the average audio segment\n",
    "audio_data = np.mean(aud_maskedData_normalized, axis=0)\n",
    "arousal_labels = arousal[:946, 1].astype('int')\n",
    "clf = SVC(C=0.01, gamma=0.001, kernel='rbf')\n",
    "classifier = clf.fit(audio_data, arousal_labels)\n",
    "arousal_pred = classifier.predict(avg_audio_segment)\n",
    "\n",
    "# Combine the predictions from the three classifiers\n",
    "timepointwise_predictions = np.vstack((inout_pred, valence_pred, arousal_pred)).T\n",
    "\n",
    "# Print the shape of the combined predictions\n",
    "print(np.shape(timepointwise_predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in the actual inout, valence, and arousal values for the TRs after 300 until the end. We then compare the classifiers' predicted values for inout, valence, and arousal from the 100 TR segment to the actualy inout, valence, and arousal at every potential start TR between 301 and 930 to see where we have the best match by getting the maximum of the set of all match accuracies between the classifier's predictions and the actual TRs between 301 and 930."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start TR is: 404\n"
     ]
    }
   ],
   "source": [
    "# Stack the actual TR values for inout, valence, and arousal\n",
    "actual_TR_values = np.vstack((inout[..., 1], valence[..., 1], arousal[..., 1])).T[301:]\n",
    "\n",
    "# Define a function to calculate the accuracy for a given TR range\n",
    "def calculate_accuracy_for_range(start_TR, predictions, actual_values):\n",
    "    correct_predictions = sum(1 for j in range(100) if all(predictions[j] == actual_values[start_TR + j]))\n",
    "    return correct_predictions / 100\n",
    "\n",
    "# Calculate accuracies for each possible TR in the range of 301 to 930\n",
    "accuracies = [calculate_accuracy_for_range(i, timepointwise_predictions, actual_TR_values) for i in range(930 - 301)]\n",
    "\n",
    "# Find the most likely TR by adding 301 to the index of the maximum accuracy\n",
    "StartTR = accuracies.index(max(accuracies)) + 301\n",
    "print('Start TR is:', StartTR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mystery Subjects \n",
    "\n",
    "You will be given a segment of 600 TRs from all participants in Part 2, but where the participant names have been scrambled. The time points and voxel orders are consistent across participants. The participants are named from A-Q and you must make your best guess as to the corresponding participant numbers. To help you with this, some participant labels have already been assigned.  \n",
    "\n",
    "This question is hard conceptually, but you have all of the tools you need. SRM will be useful but you are welcome to take any valid approach from class. Note that SRM is probabilistic so we highly suggest using a random seed. If you can accurately match the mystery subject labels for A and Q (the subjects we number below) then your analysis is probably on the right track. If you can get at least 4 other mystery subject labels correct (6/17 total), you will receive full credit for the answer component of the grade.  \n",
    "\n",
    "**Data directory:** The data can be found in: `/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/`  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the mystery subject labels \n",
    "Explain how you got to this answer and note if you are only confident on a few subjects, explaining why this may be."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** 6         \n",
    "**B:**   \n",
    "**C:**   \n",
    "**D:**  \n",
    "**E:**  \n",
    "**F:**  \n",
    "**G:**  \n",
    "**H:**  \n",
    "**I:**     \n",
    "**J:**   \n",
    "**K:**  \n",
    "**L:**  \n",
    "**M:**    \n",
    "**N:**  \n",
    "**O:**     \n",
    "**P:**  \n",
    "**Q:** 4 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include your code below\n",
    "Please make sure that your code is commented. If you performed analyses outside of this notebook (e.g., in a python script that needed more computational resources) please note that here and be sure to upload any accompanying notebook(s) or script(s) (submissions without accompanying code will be given zero points)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### None of my approaches worked but here is what I had thus far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sherlock_movie_Q.nii.gz', 'sherlock_movie_A.nii.gz', 'sherlock_movie_F.nii.gz', 'sherlock_movie_G.nii.gz', 'sherlock_movie_K.nii.gz', 'sherlock_movie_P.nii.gz', 'sherlock_movie_O.nii.gz', 'sherlock_movie_M.nii.gz', 'sherlock_movie_J.nii.gz', 'sherlock_movie_E.nii.gz', 'sherlock_movie_L.nii.gz', 'sherlock_movie_D.nii.gz', 'sherlock_movie_I.nii.gz', 'sherlock_movie_C.nii.gz', 'sherlock_movie_H.nii.gz', 'sherlock_movie_B.nii.gz', 'sherlock_movie_N.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "pt4_dir = data_dir + 'mystery_subjects_q4/'\n",
    "print(os.listdir(pt4_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = ['Q', 'A', 'F', 'G', 'K', 'P', 'O', 'M', 'J', 'E', 'L', 'D', 'I', 'C', 'H', 'B','N']\n",
    "\n",
    "p4files = [f\"{pt4_dir}sherlock_movie_{subject_id}.nii.gz\" for subject_id in subject_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_Q.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_A.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_F.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_G.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_K.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_P.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_O.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_M.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_J.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_E.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_L.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_D.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_I.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_C.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_H.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_B.nii.gz',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_subjects_q4/sherlock_movie_N.nii.gz']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-12_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-17_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-06_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-11_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-03_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-09_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-04_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-16_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-05_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-14_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-07_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-10_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-08_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-13_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-02_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-01_segment_q2.nii',\n",
       " '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-15_segment_q2.nii']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_preprocess_data(data, mask, num_subs):\n",
    "    \"\"\"Mask and preprocess data for all subjects.\n",
    "\n",
    "    Params:\n",
    "    data: 4D fMRI data\n",
    "    mask: boolean mask\n",
    "    num_subs: number of subjects\n",
    "\n",
    "    Returns:\n",
    "    all_subject_data\n",
    "    \"\"\"\n",
    "    all_subject_data = []\n",
    "    for subject in range(num_subs):\n",
    "        masked_data = image.mask_image(data[subject], mask)\n",
    "        scaler = preprocessing.StandardScaler().fit(masked_data)\n",
    "        preprocessed_data = scaler.transform(masked_data)\n",
    "        all_subject_data.append(preprocessed_data)\n",
    "    return all_subject_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from brainiak.funcalign import srm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "subjects_data_q2 = []\n",
    "subjects_data_q4 = []\n",
    "\n",
    "# Load the NIfTI images for subjects 1 to 17 and append them to subjects_data\n",
    "for i in range(1, 18):\n",
    "    nifti_image = nib.load(f'/gpfs/gibbs/project/cmhn/data/Sherlock_processed/mystery_segments_q2/sub-{i:02d}_segment_q2.nii')\n",
    "    subjects_data_q2.append(nifti_image)\n",
    "\n",
    "print(1)\n",
    "\n",
    "subject_ids = ['Q', 'A', 'F', 'G', 'K', 'P', 'O', 'M', 'J', 'E', 'L', 'D', 'I', 'C', 'H', 'B','N']\n",
    "for subject_id in subject_ids:\n",
    "    nifti_image = nib.load(f\"{pt4_dir}sherlock_movie_{subject_id}.nii.gz\")\n",
    "    subjects_data_q4.append(nifti_image)\n",
    "\n",
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "p4_data_preprocessed = mask_preprocess_data(subjects_data_q4, visual_mask, 17)\n",
    "p2_data_preprocessed = mask_preprocess_data(subjects_data_q2, visual_mask, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 307, 600)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(p4_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_data_preprocessed = p4_data_preprocessed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of subjects does not match the one in the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13574/2960500471.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Transform Q_data and the test set of the current subject using the trained SRM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mp4_shared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp4_data_preprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/brainiak/funcalign/srm.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Check the number of subjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             raise ValueError(\"The number of subjects does not match the one\"\n\u001b[0m\u001b[1;32m    294\u001b[0m                              \" in the model.\")\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of subjects does not match the one in the model."
     ]
    }
   ],
   "source": [
    "# Set up cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(1)\n",
    "max_correlation = 0\n",
    "subject_index = -1\n",
    "\n",
    "for i in range(1, 18):\n",
    "    if i == 16:  # Skip the 16th subject (sub1 with 30 TRs)\n",
    "        continue\n",
    "\n",
    "    correlations = []\n",
    "\n",
    "    for train_indices, test_indices in kf.split(p2_data_preprocessed[i - 1]):\n",
    "        # Train SRM on the training set\n",
    "        srm_model = srm.SRM(n_iter=10, features=50)\n",
    "        train_data = [p2_data_preprocessed[sub_idx][train_indices] for sub_idx in range(17) if sub_idx != i-1 and sub_idx != 15]\n",
    "        srm_model.fit(train_data)\n",
    "        print(2)\n",
    "\n",
    "        # Transform Q_data and the test set of the current subject using the trained SRM\n",
    "        p4_shared = srm_model.transform([p4_data_preprocessed])[0]\n",
    "\n",
    "\n",
    "print(f\"Q is most likely subject {subject_index}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are a few prior erroneous approaches that also didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == \"\":\n",
      "/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q is most likely subject 14 (excluding subject 16).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def load_nii_data(filepath):\n",
    "    nii = nib.load(filepath)\n",
    "    return np.array(nii.dataobj)\n",
    "\n",
    "def calculate_mean_correlation(data1, data2):\n",
    "    if data1.shape != data2.shape:\n",
    "        raise ValueError(\"The dimensions of the data arrays do not match.\")\n",
    "    \n",
    "    data1_normalized = (data1 - data1.mean(axis=-1, keepdims=True)) / data1.std(axis=-1, keepdims=True)\n",
    "    data2_normalized = (data2 - data2.mean(axis=-1, keepdims=True)) / data2.std(axis=-1, keepdims=True)\n",
    "    correlations = np.nanmean(data1_normalized * data2_normalized, axis=-1)\n",
    "\n",
    "    return np.nanmean(correlations)\n",
    "\n",
    "Q_data = load_nii_data(Qfile)[:,:,:,:300]  # Load first 300 TRs of Qfile\n",
    "max_correlation = 0\n",
    "subject_index = -1\n",
    "\n",
    "for i in range(1, 18):  # Change the loop range to include the 17th subject\n",
    "    if i == 16:  # Skip the 16th subject\n",
    "        continue\n",
    "    p2file = p2files[i-1]  # Get the file path for the current subject\n",
    "    p2_data = load_nii_data(p2file)\n",
    "    correlation = calculate_mean_correlation(Q_data, p2_data)\n",
    "\n",
    "    if correlation > max_correlation:\n",
    "        max_correlation = correlation\n",
    "        subject_index = i\n",
    "\n",
    "print(f\"Q is most likely subject {subject_index} (excluding subject 16).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == \"\":\n",
      "/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "/gpfs/gibbs/project/cmhn/share/conda_envs/mybrainiak/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is most likely subject 14 (excluding subject 16).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "A_data = load_nii_data(Afile)[:,:,:,:300]  # Load first 300 TRs of Qfile\n",
    "max_correlation = 0\n",
    "subject_index = -1\n",
    "\n",
    "for i in range(1, 18):  # Change the loop range to include the 17th subject\n",
    "    if i == 16:  # Skip the 16th subject\n",
    "        continue\n",
    "    p2file = p2files[i-1]  # Get the file path for the current subject\n",
    "    p2_data = load_nii_data(p2file)\n",
    "    correlation = calculate_mean_correlation(Q_data, p2_data)\n",
    "\n",
    "    if correlation > max_correlation:\n",
    "        max_correlation = correlation\n",
    "        subject_index = i\n",
    "\n",
    "print(f\"A is most likely subject {subject_index} (excluding subject 16).\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help\n",
    "\n",
    "This section contains helper functions and hints that you could use to solve the questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading excel files.**\n",
    "Pandas provides a nice interface to load excel files into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_dir ='/gpfs/gibbs/project/cmhn/data/Sherlock_processed/'\n",
    "df = pd.read_csv(data_dir + 'regressor_file.csv') # pandas plays best with csv files\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Extracting a column**\n",
    "To extract a column of values from a dataframe, the following sample code is provided. \n",
    "\n",
    "Be careful of any leading or trailing spaces in the column titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start= df['Start TR']\n",
    "print(start[:10]) # Just show the first 10 rows.\n",
    "\n",
    "# To turn the column into a vector of values:\n",
    "print(start.values[:10]) # Just show the first 10 values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Start TR jumps from TR 0 to TR 8 -- meaning, the regressors don't have one row for every TR, like we've seen for other datasets. You need to handle this to properly train your classifiers!*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masking data:** You will find that extracting data from a mask takes time and is memory intensive; repeating that process is unnecessary. You can apply the given masks to your data and save the masked numpy arrays in a directory in `~/palmer_scratch` if you prefer, then load that data back in whenever you want to use it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions:** We provided many functions in `utils.py` that help with data loading and normalization. Use them. Feel free to create your own `final_project_utils.py` with any paths, functions, etc. that you create and reuse for this project. Upload that along with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from brainiak import image, io\n",
    "from scipy.stats import stats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the PMC mask\n",
    "from brainiak import image, io\n",
    "data_dir = '/gpfs/gibbs/project/cmhn/data/Sherlock_processed/'\n",
    "mask_file= data_dir + 'masks/pmc_nn.nii'\n",
    "pmc_mask = io.load_boolean_mask(mask_file)\n",
    "plt.imshow(pmc_mask[:,:,30]) #30th Z slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the whole brain mask\n",
    "mask_file= data_dir + 'masks/MNI152_T1_3mm_brain_mask.nii'\n",
    "brain_mask = io.load_boolean_mask(mask_file)\n",
    "plt.imshow(brain_mask[:,:,30]) #30th Z slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the intersect mask -- where all subjects have the same voxels present\n",
    "mask_file= data_dir + 'masks/intersect_mask.nii.gz'\n",
    "intersect_mask = io.load_boolean_mask(mask_file)\n",
    "plt.imshow(intersect_mask[:,:,30]) #30th Z slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
